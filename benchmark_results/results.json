{
  "generated_at": "2026-02-21T04:02:32.308067+00:00",
  "n_queries": 35,
  "approaches": [
    "Hybrid",
    "BM25",
    "Semantic",
    "LLM-Rerank"
  ],
  "per_query": [
    {
      "query": "Who is Ramesh Raskar?",
      "level": "L1",
      "scoring": "normal",
      "keywords": [
        "ramesh",
        "raskar",
        "professor",
        "mit"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "BM25": {
          "recall": 0.75,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.75,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "Semantic": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        }
      }
    },
    {
      "query": "Are there any course videos?",
      "level": "L1",
      "scoring": "normal",
      "keywords": [
        "video",
        "youtube"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 1.0,
          "mrr": 0.3333333333333333,
          "ndcg": 0.5,
          "effective_recall": 1.0,
          "effective_mrr": 0.3333333333333333,
          "effective_ndcg": 0.5,
          "passed": true
        },
        "BM25": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "Semantic": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        }
      }
    },
    {
      "query": "What is the course about?",
      "level": "L1",
      "scoring": "normal",
      "keywords": [
        "ai",
        "venture",
        "prototyping",
        "hands-on"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.5,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.5,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "BM25": {
          "recall": 0.75,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.75,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "Semantic": {
          "recall": 0.75,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.75,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.8919478197221425,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8919478197221425,
          "passed": true
        }
      }
    },
    {
      "query": "How do I register for the course?",
      "level": "L2",
      "scoring": "normal",
      "keywords": [
        "register",
        "questionnaire",
        "apply",
        "form"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.75,
          "mrr": 0.3333333333333333,
          "ndcg": 0.5976315487360696,
          "effective_recall": 0.75,
          "effective_mrr": 0.3333333333333333,
          "effective_ndcg": 0.5976315487360696,
          "passed": true
        },
        "BM25": {
          "recall": 0.75,
          "mrr": 1.0,
          "ndcg": 0.8986776544421414,
          "effective_recall": 0.75,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8986776544421414,
          "passed": true
        },
        "Semantic": {
          "recall": 0.75,
          "mrr": 0.3333333333333333,
          "ndcg": 0.4573369727327457,
          "effective_recall": 0.75,
          "effective_mrr": 0.3333333333333333,
          "effective_ndcg": 0.4573369727327457,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.75,
          "mrr": 1.0,
          "ndcg": 0.9121656860366328,
          "effective_recall": 0.75,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9121656860366328,
          "passed": true
        }
      }
    },
    {
      "query": "What is the course format for Fall 2025?",
      "level": "L2",
      "scoring": "normal",
      "keywords": [
        "venture studio",
        "agentic",
        "demo day"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.6666666666666666,
          "mrr": 1.0,
          "ndcg": 0.9920466006454152,
          "effective_recall": 0.6666666666666666,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9920466006454152,
          "passed": true
        },
        "BM25": {
          "recall": 0.6666666666666666,
          "mrr": 1.0,
          "ndcg": 0.971408902537385,
          "effective_recall": 0.6666666666666666,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.971408902537385,
          "passed": true
        },
        "Semantic": {
          "recall": 0.6666666666666666,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.6666666666666666,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.6666666666666666,
          "mrr": 1.0,
          "ndcg": 0.866107201235062,
          "effective_recall": 0.6666666666666666,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.866107201235062,
          "passed": true
        }
      }
    },
    {
      "query": "What venture capital firms are involved as speakers or mentors?",
      "level": "L3",
      "scoring": "normal",
      "keywords": [
        "khosla",
        "lux",
        "link ventures",
        "two lanterns",
        "pillar",
        "e14"
      ],
      "top_k": 15,
      "approaches": {
        "Hybrid": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.7671980430548917,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7671980430548917,
          "passed": true
        },
        "BM25": {
          "recall": 0.8333333333333334,
          "mrr": 1.0,
          "ndcg": 0.9827994861811935,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9827994861811935,
          "passed": true
        },
        "Semantic": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.5531239834838553,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.5531239834838553,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.7033736910334266,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7033736910334266,
          "passed": true
        }
      }
    },
    {
      "query": "Who were the venture capital speakers in Fall 2023?",
      "level": "L3",
      "scoring": "normal",
      "keywords": [
        "khosla",
        "lux",
        "link",
        "e14",
        "pillar"
      ],
      "top_k": 15,
      "approaches": {
        "Hybrid": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.6022440725213158,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.6022440725213158,
          "passed": true
        },
        "BM25": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.9595981238976526,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9595981238976526,
          "passed": true
        },
        "Semantic": {
          "recall": 0.2,
          "mrr": 1.0,
          "ndcg": 0.8221120471878671,
          "effective_recall": 0.2,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8221120471878671,
          "passed": false
        },
        "LLM-Rerank": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.8791456368861218,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8791456368861218,
          "passed": true
        }
      }
    },
    {
      "query": "How has the course name changed across semesters?",
      "level": "L3",
      "scoring": "normal",
      "keywords": [
        "web3",
        "venture",
        "agentic",
        "foundations"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.75,
          "mrr": 1.0,
          "ndcg": 0.8442355845243696,
          "effective_recall": 0.75,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8442355845243696,
          "passed": true
        },
        "BM25": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "Semantic": {
          "recall": 0.75,
          "mrr": 1.0,
          "ndcg": 0.6776636931349654,
          "effective_recall": 0.75,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.6776636931349654,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.75,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.75,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        }
      }
    },
    {
      "query": "What changed between the 2025 and 2026 versions of the course?",
      "level": "L3",
      "scoring": "normal",
      "keywords": [
        "agentic",
        "autonomous",
        "venture studio",
        "foundations",
        "spring 2026",
        "spring 2025"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.5,
          "mrr": 1.0,
          "ndcg": 0.8401498110374593,
          "effective_recall": 0.5,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8401498110374593,
          "passed": true
        },
        "BM25": {
          "recall": 0.5,
          "mrr": 0.5,
          "ndcg": 0.567207416956871,
          "effective_recall": 0.5,
          "effective_mrr": 0.5,
          "effective_ndcg": 0.567207416956871,
          "passed": true
        },
        "Semantic": {
          "recall": 0.5,
          "mrr": 1.0,
          "ndcg": 0.8447122853992027,
          "effective_recall": 0.5,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8447122853992027,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.3333333333333333,
          "mrr": 1.0,
          "ndcg": 0.8599797111848092,
          "effective_recall": 0.3333333333333333,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8599797111848092,
          "passed": false
        }
      }
    },
    {
      "query": "What is NANDA and why does Raskar think it matters?",
      "level": "L4",
      "scoring": "normal",
      "keywords": [
        "nanda",
        "network",
        "agent",
        "decentralized",
        "internet"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.6673385207601783,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.6673385207601783,
          "passed": true
        },
        "BM25": {
          "recall": 0.4,
          "mrr": 1.0,
          "ndcg": 0.8223779611386884,
          "effective_recall": 0.4,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8223779611386884,
          "passed": true
        },
        "Semantic": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.9735792424866951,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9735792424866951,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.9359980548891793,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9359980548891793,
          "passed": true
        }
      }
    },
    {
      "query": "What did Raskar say about privacy and decentralized AI?",
      "level": "L4",
      "scoring": "normal",
      "keywords": [
        "privacy",
        "decentralized",
        "data",
        "machine learning"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.9629027936988324,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9629027936988324,
          "passed": true
        },
        "BM25": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.9488136009556424,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9488136009556424,
          "passed": true
        },
        "Semantic": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.9629027936988324,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9629027936988324,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        }
      }
    },
    {
      "query": "What companies like Mitsubishi were judges or partners at Demo Day?",
      "level": "L5",
      "scoring": "normal",
      "keywords": [
        "mitsubishi",
        "judge",
        "partner",
        "corporate"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.75,
          "mrr": 1.0,
          "ndcg": 0.7770227215242603,
          "effective_recall": 0.75,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7770227215242603,
          "passed": true
        },
        "BM25": {
          "recall": 0.75,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.75,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "Semantic": {
          "recall": 0.75,
          "mrr": 1.0,
          "ndcg": 0.7979168917078058,
          "effective_recall": 0.75,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7979168917078058,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.5,
          "mrr": 1.0,
          "ndcg": 0.8447122853992027,
          "effective_recall": 0.5,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8447122853992027,
          "passed": true
        }
      }
    },
    {
      "query": "How does Raskar compare the internet evolution to the agentic web?",
      "level": "L5",
      "scoring": "normal",
      "keywords": [
        "internet",
        "worldwide web",
        "mainframe",
        "intranet",
        "agentic"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.6,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.6,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "BM25": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "Semantic": {
          "recall": 0.8,
          "mrr": 1.0,
          "ndcg": 0.7521728881423634,
          "effective_recall": 0.8,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7521728881423634,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.8,
          "mrr": 1.0,
          "ndcg": 0.9338511340819263,
          "effective_recall": 0.8,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9338511340819263,
          "passed": true
        }
      }
    },
    {
      "query": "What healthcare or medical examples did speakers discuss?",
      "level": "L5",
      "scoring": "normal",
      "keywords": [
        "health",
        "medical",
        "patient",
        "hospital",
        "diabetic",
        "chest"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.5,
          "mrr": 1.0,
          "ndcg": 0.7521866188906461,
          "effective_recall": 0.5,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7521866188906461,
          "passed": true
        },
        "BM25": {
          "recall": 0.6666666666666666,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.6666666666666666,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "Semantic": {
          "recall": 0.5,
          "mrr": 1.0,
          "ndcg": 0.7521866188906461,
          "effective_recall": 0.5,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7521866188906461,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.6666666666666666,
          "mrr": 1.0,
          "ndcg": 0.9413225986683531,
          "effective_recall": 0.6666666666666666,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9413225986683531,
          "passed": true
        }
      }
    },
    {
      "query": "Who funds early-stage startups at the course?",
      "level": "L6",
      "scoring": "normal",
      "keywords": [
        "khosla",
        "lux",
        "link ventures",
        "e14",
        "pillar",
        "venture"
      ],
      "top_k": 15,
      "approaches": {
        "Hybrid": {
          "recall": 0.8333333333333334,
          "mrr": 1.0,
          "ndcg": 0.8174152298079348,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8174152298079348,
          "passed": true
        },
        "BM25": {
          "recall": 0.8333333333333334,
          "mrr": 1.0,
          "ndcg": 0.8307469081123,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8307469081123,
          "passed": true
        },
        "Semantic": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.6838554026395006,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.6838554026395006,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.8412934800411139,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8412934800411139,
          "passed": true
        }
      }
    },
    {
      "query": "What cutting-edge tech topics are covered in lectures?",
      "level": "L6",
      "scoring": "normal",
      "keywords": [
        "agentic",
        "autonomous",
        "nanda",
        "decentralized",
        "web3",
        "blockchain"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.0,
          "mrr": 0.0,
          "ndcg": 1.0,
          "effective_recall": 0.0,
          "effective_mrr": 0.0,
          "effective_ndcg": 1.0,
          "passed": false
        },
        "BM25": {
          "recall": 0.16666666666666666,
          "mrr": 0.2,
          "ndcg": 0.38685280723454163,
          "effective_recall": 0.16666666666666666,
          "effective_mrr": 0.2,
          "effective_ndcg": 0.38685280723454163,
          "passed": false
        },
        "Semantic": {
          "recall": 0.0,
          "mrr": 0.0,
          "ndcg": 1.0,
          "effective_recall": 0.0,
          "effective_mrr": 0.0,
          "effective_ndcg": 1.0,
          "passed": false
        },
        "LLM-Rerank": {
          "recall": 0.5,
          "mrr": 0.5,
          "ndcg": 0.6959964588011701,
          "effective_recall": 0.5,
          "effective_mrr": 0.5,
          "effective_ndcg": 0.6959964588011701,
          "passed": true
        }
      }
    },
    {
      "query": "Who helps students refine their business ideas?",
      "level": "L6",
      "scoring": "normal",
      "keywords": [
        "mentor",
        "speaker",
        "instructor",
        "co-instructor",
        "judge",
        "catalyst"
      ],
      "top_k": 10,
      "approaches": {
        "Hybrid": {
          "recall": 0.6666666666666666,
          "mrr": 0.3333333333333333,
          "ndcg": 0.6126858339449328,
          "effective_recall": 0.6666666666666666,
          "effective_mrr": 0.3333333333333333,
          "effective_ndcg": 0.6126858339449328,
          "passed": true
        },
        "BM25": {
          "recall": 0.8333333333333334,
          "mrr": 1.0,
          "ndcg": 0.7527035338965439,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7527035338965439,
          "passed": true
        },
        "Semantic": {
          "recall": 0.6666666666666666,
          "mrr": 0.3333333333333333,
          "ndcg": 0.5106716842493384,
          "effective_recall": 0.6666666666666666,
          "effective_mrr": 0.3333333333333333,
          "effective_ndcg": 0.5106716842493384,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.5,
          "mrr": 1.0,
          "ndcg": 0.7567348568360622,
          "effective_recall": 0.5,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7567348568360622,
          "passed": true
        }
      }
    },
    {
      "query": "How do teams show off their work at the end of the semester?",
      "level": "L6",
      "scoring": "normal",
      "keywords": [
        "demo day",
        "pitch",
        "presentation",
        "showcase",
        "judge"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.8241377469002206,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8241377469002206,
          "passed": true
        },
        "BM25": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.8056979221630077,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8056979221630077,
          "passed": true
        },
        "Semantic": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.8241377469002206,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8241377469002206,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.9801522273172875,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9801522273172875,
          "passed": true
        }
      }
    },
    {
      "query": "What privacy-preserving machine learning method was invented at MIT?",
      "level": "L6",
      "scoring": "normal",
      "keywords": [
        "split learning",
        "federated",
        "privacy",
        "no peak",
        "decentralized"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.4,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.4,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "BM25": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.9720982957292578,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9720982957292578,
          "passed": true
        },
        "Semantic": {
          "recall": 0.4,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.4,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.4,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.4,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        }
      }
    },
    {
      "query": "Which speakers or mentors appeared in both Fall 2023 and Spring 2024?",
      "level": "L7",
      "scoring": "normal",
      "keywords": [
        "raskar",
        "ramesh",
        "john",
        "werner",
        "habib",
        "hadad",
        "link"
      ],
      "top_k": 15,
      "approaches": {
        "Hybrid": {
          "recall": 0.42857142857142855,
          "mrr": 0.125,
          "ndcg": 0.3458369414795075,
          "effective_recall": 0.42857142857142855,
          "effective_mrr": 0.125,
          "effective_ndcg": 0.3458369414795075,
          "passed": true
        },
        "BM25": {
          "recall": 0.8571428571428571,
          "mrr": 1.0,
          "ndcg": 0.919399865400546,
          "effective_recall": 0.8571428571428571,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.919399865400546,
          "passed": true
        },
        "Semantic": {
          "recall": 0.2857142857142857,
          "mrr": 0.07692307692307693,
          "ndcg": 0.360691176496464,
          "effective_recall": 0.2857142857142857,
          "effective_mrr": 0.07692307692307693,
          "effective_ndcg": 0.360691176496464,
          "passed": false
        },
        "LLM-Rerank": {
          "recall": 0.2857142857142857,
          "mrr": 1.0,
          "ndcg": 0.7681402000657983,
          "effective_recall": 0.2857142857142857,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7681402000657983,
          "passed": false
        }
      }
    },
    {
      "query": "How has the number of student teams changed across semesters?",
      "level": "L7",
      "scoring": "normal",
      "keywords": [
        "15",
        "24",
        "38",
        "teams",
        "presenting"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.4,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.4,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "BM25": {
          "recall": 0.8,
          "mrr": 1.0,
          "ndcg": 0.9069783029220914,
          "effective_recall": 0.8,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9069783029220914,
          "passed": true
        },
        "Semantic": {
          "recall": 0.4,
          "mrr": 1.0,
          "ndcg": 0.9605753368118397,
          "effective_recall": 0.4,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9605753368118397,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.4,
          "mrr": 1.0,
          "ndcg": 0.8558112355261802,
          "effective_recall": 0.4,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8558112355261802,
          "passed": true
        }
      }
    },
    {
      "query": "What real-world companies were both judges and corporate partners at demo days?",
      "level": "L7",
      "scoring": "normal",
      "keywords": [
        "mitsubishi",
        "state street",
        "ey",
        "mass challenge"
      ],
      "top_k": 15,
      "approaches": {
        "Hybrid": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.8011166442354041,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8011166442354041,
          "passed": true
        },
        "BM25": {
          "recall": 0.25,
          "mrr": 0.5,
          "ndcg": 0.8663422763540115,
          "effective_recall": 0.25,
          "effective_mrr": 0.5,
          "effective_ndcg": 0.8663422763540115,
          "passed": false
        },
        "Semantic": {
          "recall": 0.5,
          "mrr": 1.0,
          "ndcg": 0.9170245493900888,
          "effective_recall": 0.5,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9170245493900888,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.5,
          "mrr": 1.0,
          "ndcg": 0.9028373627369358,
          "effective_recall": 0.5,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9028373627369358,
          "passed": true
        }
      }
    },
    {
      "query": "Did the course evaluation criteria change between Spring 2023 and Fall 2025?",
      "level": "L7",
      "scoring": "normal",
      "keywords": [
        "impact",
        "unique",
        "complete",
        "demo",
        "judge",
        "gigas scale"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.8333333333333334,
          "mrr": 1.0,
          "ndcg": 0.8080267481237329,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8080267481237329,
          "passed": true
        },
        "BM25": {
          "recall": 0.3333333333333333,
          "mrr": 1.0,
          "ndcg": 0.7960767356132932,
          "effective_recall": 0.3333333333333333,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7960767356132932,
          "passed": false
        },
        "Semantic": {
          "recall": 0.8333333333333334,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.3333333333333333,
          "mrr": 1.0,
          "ndcg": 0.9467009784197935,
          "effective_recall": 0.3333333333333333,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9467009784197935,
          "passed": false
        }
      }
    },
    {
      "query": "What is the tuition cost for the AI Studio course?",
      "level": "L8",
      "scoring": "inverse",
      "keywords": [
        "tuition",
        "cost",
        "fee",
        "price",
        "dollar",
        "pay"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.5,
          "mrr": 0.5,
          "ndcg": 0.6728853390803203,
          "effective_recall": 0.5,
          "effective_mrr": 0.5,
          "effective_ndcg": 0.32711466091967967,
          "passed": true
        },
        "BM25": {
          "recall": 0.5,
          "mrr": 1.0,
          "ndcg": 0.9238850086262383,
          "effective_recall": 0.5,
          "effective_mrr": 0.0,
          "effective_ndcg": 0.07611499137376165,
          "passed": true
        },
        "Semantic": {
          "recall": 0.5,
          "mrr": 0.5,
          "ndcg": 0.6728853390803203,
          "effective_recall": 0.5,
          "effective_mrr": 0.5,
          "effective_ndcg": 0.32711466091967967,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.0,
          "mrr": 0.0,
          "ndcg": 1.0,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.0,
          "passed": true
        }
      }
    },
    {
      "query": "What programming languages are required as prerequisites?",
      "level": "L8",
      "scoring": "inverse",
      "keywords": [
        "python",
        "java",
        "javascript",
        "prerequisite",
        "requirement",
        "coding"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.16666666666666666,
          "mrr": 0.5,
          "ndcg": 0.6934264036172707,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 0.5,
          "effective_ndcg": 0.3065735963827293,
          "passed": true
        },
        "BM25": {
          "recall": 0.0,
          "mrr": 0.0,
          "ndcg": 1.0,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.0,
          "passed": true
        },
        "Semantic": {
          "recall": 0.16666666666666666,
          "mrr": 0.5,
          "ndcg": 0.6934264036172707,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 0.5,
          "effective_ndcg": 0.3065735963827293,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.16666666666666666,
          "mrr": 0.5,
          "ndcg": 0.6934264036172707,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 0.5,
          "effective_ndcg": 0.3065735963827293,
          "passed": true
        }
      }
    },
    {
      "query": "What is the final exam format for the course?",
      "level": "L8",
      "scoring": "inverse",
      "keywords": [
        "exam",
        "test",
        "midterm",
        "final",
        "quiz",
        "grading"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.16666666666666666,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 0.0,
          "effective_ndcg": 0.0,
          "passed": true
        },
        "BM25": {
          "recall": 0.16666666666666666,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 0.0,
          "effective_ndcg": 0.0,
          "passed": true
        },
        "Semantic": {
          "recall": 0.16666666666666666,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 0.0,
          "effective_ndcg": 0.0,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.16666666666666666,
          "mrr": 0.25,
          "ndcg": 0.5012658353418871,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 0.75,
          "effective_ndcg": 0.49873416465811293,
          "passed": true
        }
      }
    },
    {
      "query": "Which VCs are no longer participating in the course this year?",
      "level": "L9",
      "scoring": "normal",
      "keywords": [
        "khosla",
        "lux",
        "link",
        "e14",
        "pillar",
        "two lanterns"
      ],
      "top_k": 15,
      "approaches": {
        "Hybrid": {
          "recall": 0.6666666666666666,
          "mrr": 1.0,
          "ndcg": 0.7031432329334039,
          "effective_recall": 0.6666666666666666,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7031432329334039,
          "passed": true
        },
        "BM25": {
          "recall": 0.5,
          "mrr": 0.5,
          "ndcg": 0.7006233574948963,
          "effective_recall": 0.5,
          "effective_mrr": 0.5,
          "effective_ndcg": 0.7006233574948963,
          "passed": true
        },
        "Semantic": {
          "recall": 0.6666666666666666,
          "mrr": 1.0,
          "ndcg": 0.6821160297746383,
          "effective_recall": 0.6666666666666666,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.6821160297746383,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.8107702033316995,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8107702033316995,
          "passed": true
        }
      }
    },
    {
      "query": "How have the demo day judging criteria evolved from Spring 2023 to Fall 2025?",
      "level": "L9",
      "scoring": "normal",
      "keywords": [
        "impact",
        "unique",
        "complete",
        "demo",
        "judge",
        "gigas scale",
        "criteria"
      ],
      "top_k": 10,
      "approaches": {
        "Hybrid": {
          "recall": 0.8571428571428571,
          "mrr": 1.0,
          "ndcg": 0.9211410805414261,
          "effective_recall": 0.8571428571428571,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9211410805414261,
          "passed": true
        },
        "BM25": {
          "recall": 0.2857142857142857,
          "mrr": 1.0,
          "ndcg": 0.9461595718627467,
          "effective_recall": 0.2857142857142857,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9461595718627467,
          "passed": false
        },
        "Semantic": {
          "recall": 0.8571428571428571,
          "mrr": 1.0,
          "ndcg": 0.9557911522471707,
          "effective_recall": 0.8571428571428571,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9557911522471707,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.922614891023392,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.922614891023392,
          "passed": true
        }
      }
    },
    {
      "query": "How has the number of student teams grown from Spring 2023 to Fall 2025?",
      "level": "L9",
      "scoring": "normal",
      "keywords": [
        "15",
        "24",
        "38",
        "teams",
        "presenting",
        "demo"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.5,
          "mrr": 1.0,
          "ndcg": 0.960960846717329,
          "effective_recall": 0.5,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.960960846717329,
          "passed": true
        },
        "BM25": {
          "recall": 0.3333333333333333,
          "mrr": 1.0,
          "ndcg": 0.7747592084729605,
          "effective_recall": 0.3333333333333333,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7747592084729605,
          "passed": false
        },
        "Semantic": {
          "recall": 0.6666666666666666,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.6666666666666666,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.3333333333333333,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.3333333333333333,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": false
        }
      }
    },
    {
      "query": "Why does Raskar say NANDA needs to involve universities and not just companies?",
      "level": "L10",
      "scoring": "normal",
      "keywords": [
        "university",
        "nanda",
        "trust",
        "decentralized",
        "neutral",
        "open"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.8333333333333334,
          "mrr": 1.0,
          "ndcg": 0.9488136009556423,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9488136009556423,
          "passed": true
        },
        "BM25": {
          "recall": 0.6666666666666666,
          "mrr": 1.0,
          "ndcg": 0.8508342748471082,
          "effective_recall": 0.6666666666666666,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8508342748471082,
          "passed": true
        },
        "Semantic": {
          "recall": 0.8333333333333334,
          "mrr": 1.0,
          "ndcg": 0.9343710124351232,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9343710124351232,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.5,
          "mrr": 1.0,
          "ndcg": 0.8508342748471082,
          "effective_recall": 0.5,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8508342748471082,
          "passed": true
        }
      }
    },
    {
      "query": "What does gigascale impact mean in the context of this course?",
      "level": "L10",
      "scoring": "normal",
      "keywords": [
        "gigas scale",
        "billion",
        "impact",
        "people",
        "scale"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.921396320854073,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.921396320854073,
          "passed": true
        },
        "BM25": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "Semantic": {
          "recall": 1.0,
          "mrr": 1.0,
          "ndcg": 0.921396320854073,
          "effective_recall": 1.0,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.921396320854073,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.8,
          "mrr": 1.0,
          "ndcg": 0.8241911108942168,
          "effective_recall": 0.8,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8241911108942168,
          "passed": true
        }
      }
    },
    {
      "query": "What is the quilt approach to building AI products that Raskar describes?",
      "level": "L10",
      "scoring": "normal",
      "keywords": [
        "quilt",
        "quilting",
        "model",
        "combine",
        "agent"
      ],
      "top_k": 5,
      "approaches": {
        "Hybrid": {
          "recall": 0.2,
          "mrr": 1.0,
          "ndcg": 0.9469024295259743,
          "effective_recall": 0.2,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9469024295259743,
          "passed": false
        },
        "BM25": {
          "recall": 0.6,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.6,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        },
        "Semantic": {
          "recall": 0.2,
          "mrr": 1.0,
          "ndcg": 0.9469024295259743,
          "effective_recall": 0.2,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9469024295259743,
          "passed": false
        },
        "LLM-Rerank": {
          "recall": 0.4,
          "mrr": 1.0,
          "ndcg": 1.0,
          "effective_recall": 0.4,
          "effective_mrr": 1.0,
          "effective_ndcg": 1.0,
          "passed": true
        }
      }
    },
    {
      "query": "Name all the companies that have ever been judges or sponsors at demo days.",
      "level": "L11",
      "scoring": "normal",
      "keywords": [
        "mitsubishi",
        "state street",
        "ey",
        "mass challenge",
        "sponsor",
        "judge"
      ],
      "top_k": 15,
      "approaches": {
        "Hybrid": {
          "recall": 0.8333333333333334,
          "mrr": 1.0,
          "ndcg": 0.8626044552122213,
          "effective_recall": 0.8333333333333334,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8626044552122213,
          "passed": true
        },
        "BM25": {
          "recall": 0.6666666666666666,
          "mrr": 1.0,
          "ndcg": 0.856517514799784,
          "effective_recall": 0.6666666666666666,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.856517514799784,
          "passed": true
        },
        "Semantic": {
          "recall": 0.5,
          "mrr": 1.0,
          "ndcg": 0.9402268036207482,
          "effective_recall": 0.5,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9402268036207482,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.5,
          "mrr": 1.0,
          "ndcg": 0.8112134259965621,
          "effective_recall": 0.5,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.8112134259965621,
          "passed": true
        }
      }
    },
    {
      "query": "What are all the healthcare or medical AI applications discussed across all semesters?",
      "level": "L11",
      "scoring": "normal",
      "keywords": [
        "health",
        "medical",
        "patient",
        "diabetic",
        "chest",
        "hospital",
        "clinical"
      ],
      "top_k": 10,
      "approaches": {
        "Hybrid": {
          "recall": 0.7142857142857143,
          "mrr": 1.0,
          "ndcg": 0.7826853730468758,
          "effective_recall": 0.7142857142857143,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7826853730468758,
          "passed": true
        },
        "BM25": {
          "recall": 0.8571428571428571,
          "mrr": 1.0,
          "ndcg": 0.9833928888197015,
          "effective_recall": 0.8571428571428571,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9833928888197015,
          "passed": true
        },
        "Semantic": {
          "recall": 0.7142857142857143,
          "mrr": 1.0,
          "ndcg": 0.7826853730468758,
          "effective_recall": 0.7142857142857143,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7826853730468758,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 0.8571428571428571,
          "mrr": 1.0,
          "ndcg": 0.9380187625423563,
          "effective_recall": 0.8571428571428571,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.9380187625423563,
          "passed": true
        }
      }
    },
    {
      "query": "Whose bio or role has changed the most across semesters of the course?",
      "level": "L12",
      "scoring": "normal",
      "keywords": [
        "werner",
        "john",
        "visiting lecturer",
        "link ventures",
        "santanu",
        "bhattacharya",
        "airtel",
        "media lab",
        "nanda",
        "chris pease"
      ],
      "top_k": 15,
      "approaches": {
        "Hybrid": {
          "recall": 0.9,
          "mrr": 0.3333333333333333,
          "ndcg": 0.4590777890428118,
          "effective_recall": 0.9,
          "effective_mrr": 0.3333333333333333,
          "effective_ndcg": 0.4590777890428118,
          "passed": true
        },
        "BM25": {
          "recall": 0.3,
          "mrr": 1.0,
          "ndcg": 0.7448938292196018,
          "effective_recall": 0.3,
          "effective_mrr": 1.0,
          "effective_ndcg": 0.7448938292196018,
          "passed": false
        },
        "Semantic": {
          "recall": 0.9,
          "mrr": 0.3333333333333333,
          "ndcg": 0.47742744029197676,
          "effective_recall": 0.9,
          "effective_mrr": 0.3333333333333333,
          "effective_ndcg": 0.47742744029197676,
          "passed": true
        },
        "LLM-Rerank": {
          "recall": 1.0,
          "mrr": 0.25,
          "ndcg": 0.4927598258751643,
          "effective_recall": 1.0,
          "effective_mrr": 0.25,
          "effective_ndcg": 0.4927598258751643,
          "passed": true
        }
      }
    }
  ],
  "aggregates": {
    "Hybrid": {
      "Recall@K": 0.7214285714285714,
      "MRR": 0.8130952380952381,
      "NDCG@K": 0.7615025384576383,
      "pass@1": 0.9428571428571428,
      "pass@3": 1.0,
      "pass^3": 0.8381807580174927,
      "pass_rate": 0.9428571428571428,
      "n_passed": 33,
      "n_total": 35
    },
    "BM25": {
      "Recall@K": 0.7052380952380952,
      "MRR": 0.8771428571428571,
      "NDCG@K": 0.8091735837264494,
      "pass@1": 0.8285714285714285,
      "pass@3": 0.9969442322383499,
      "pass^3": 0.5688396501457728,
      "pass_rate": 0.8285714285714286,
      "n_passed": 29,
      "n_total": 35
    },
    "Semantic": {
      "Recall@K": 0.6930612244897959,
      "MRR": 0.8307692307692308,
      "NDCG@K": 0.7750076609271834,
      "pass@1": 0.8857142857142857,
      "pass@3": 0.99938884644767,
      "pass^3": 0.6948338192419824,
      "pass_rate": 0.8857142857142857,
      "n_passed": 31,
      "n_total": 35
    },
    "LLM-Rerank": {
      "Recall@K": 0.7269387755102041,
      "MRR": 0.9428571428571428,
      "NDCG@K": 0.827770882126644,
      "pass@1": 0.8857142857142857,
      "pass@3": 0.99938884644767,
      "pass^3": 0.6948338192419824,
      "pass_rate": 0.8857142857142857,
      "n_passed": 31,
      "n_total": 35
    }
  },
  "per_level": {
    "L1": {
      "n": 3,
      "approaches": {
        "Hybrid": {
          "mean_recall": 0.8333333333333334,
          "mean_mrr": 0.7777777777777778,
          "mean_ndcg": 0.8333333333333334
        },
        "BM25": {
          "mean_recall": 0.8333333333333334,
          "mean_mrr": 1.0,
          "mean_ndcg": 1.0
        },
        "Semantic": {
          "mean_recall": 0.9166666666666666,
          "mean_mrr": 1.0,
          "mean_ndcg": 1.0
        },
        "LLM-Rerank": {
          "mean_recall": 1.0,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.9639826065740476
        }
      }
    },
    "L2": {
      "n": 2,
      "approaches": {
        "Hybrid": {
          "mean_recall": 0.7083333333333333,
          "mean_mrr": 0.6666666666666666,
          "mean_ndcg": 0.7948390746907423
        },
        "BM25": {
          "mean_recall": 0.7083333333333333,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.9350432784897632
        },
        "Semantic": {
          "mean_recall": 0.7083333333333333,
          "mean_mrr": 0.6666666666666666,
          "mean_ndcg": 0.7286684863663728
        },
        "LLM-Rerank": {
          "mean_recall": 0.7083333333333333,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.8891364436358474
        }
      }
    },
    "L3": {
      "n": 4,
      "approaches": {
        "Hybrid": {
          "mean_recall": 0.8125,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.7634568777845091
        },
        "BM25": {
          "mean_recall": 0.8333333333333334,
          "mean_mrr": 0.875,
          "mean_ndcg": 0.8774012567589293
        },
        "Semantic": {
          "mean_recall": 0.6125,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.7244030023014727
        },
        "LLM-Rerank": {
          "mean_recall": 0.7708333333333334,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.8606247597760894
        }
      }
    },
    "L4": {
      "n": 2,
      "approaches": {
        "Hybrid": {
          "mean_recall": 1.0,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.8151206572295053
        },
        "BM25": {
          "mean_recall": 0.7,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.8855957810471654
        },
        "Semantic": {
          "mean_recall": 1.0,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.9682410180927637
        },
        "LLM-Rerank": {
          "mean_recall": 1.0,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.9679990274445897
        }
      }
    },
    "L5": {
      "n": 3,
      "approaches": {
        "Hybrid": {
          "mean_recall": 0.6166666666666667,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.8430697801383021
        },
        "BM25": {
          "mean_recall": 0.8055555555555555,
          "mean_mrr": 1.0,
          "mean_ndcg": 1.0
        },
        "Semantic": {
          "mean_recall": 0.6833333333333332,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.7674254662469385
        },
        "LLM-Rerank": {
          "mean_recall": 0.6555555555555556,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.906628672716494
        }
      }
    },
    "L6": {
      "n": 5,
      "approaches": {
        "Hybrid": {
          "mean_recall": 0.58,
          "mean_mrr": 0.6666666666666667,
          "mean_ndcg": 0.8508477621306177
        },
        "BM25": {
          "mean_recall": 0.7666666666666667,
          "mean_mrr": 0.8400000000000001,
          "mean_ndcg": 0.7496198934271302
        },
        "Semantic": {
          "mean_recall": 0.6133333333333333,
          "mean_mrr": 0.6666666666666667,
          "mean_ndcg": 0.8037329667578119
        },
        "LLM-Rerank": {
          "mean_recall": 0.6799999999999999,
          "mean_mrr": 0.9,
          "mean_ndcg": 0.8548354045991268
        }
      }
    },
    "L7": {
      "n": 4,
      "approaches": {
        "Hybrid": {
          "mean_recall": 0.6654761904761904,
          "mean_mrr": 0.78125,
          "mean_ndcg": 0.7387450834596612
        },
        "BM25": {
          "mean_recall": 0.5601190476190476,
          "mean_mrr": 0.875,
          "mean_ndcg": 0.8721992950724855
        },
        "Semantic": {
          "mean_recall": 0.5047619047619047,
          "mean_mrr": 0.7692307692307693,
          "mean_ndcg": 0.8095727656745981
        },
        "LLM-Rerank": {
          "mean_recall": 0.37976190476190474,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.868372444187177
        }
      }
    },
    "L8": {
      "n": 3,
      "approaches": {
        "Hybrid": {
          "mean_recall": 0.7222222222222223,
          "mean_mrr": 0.3333333333333333,
          "mean_ndcg": 0.21122941910080298
        },
        "BM25": {
          "mean_recall": 0.7777777777777778,
          "mean_mrr": 0.3333333333333333,
          "mean_ndcg": 0.025371663791253884
        },
        "Semantic": {
          "mean_recall": 0.7222222222222223,
          "mean_mrr": 0.3333333333333333,
          "mean_ndcg": 0.21122941910080298
        },
        "LLM-Rerank": {
          "mean_recall": 0.888888888888889,
          "mean_mrr": 0.75,
          "mean_ndcg": 0.2684359203469474
        }
      }
    },
    "L9": {
      "n": 3,
      "approaches": {
        "Hybrid": {
          "mean_recall": 0.6746031746031745,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.8617483867307195
        },
        "BM25": {
          "mean_recall": 0.373015873015873,
          "mean_mrr": 0.8333333333333334,
          "mean_ndcg": 0.8071807126102012
        },
        "Semantic": {
          "mean_recall": 0.7301587301587301,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.8793023940072696
        },
        "LLM-Rerank": {
          "mean_recall": 0.7777777777777778,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.9111283647850305
        }
      }
    },
    "L10": {
      "n": 3,
      "approaches": {
        "Hybrid": {
          "mean_recall": 0.6777777777777777,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.9390374504452298
        },
        "BM25": {
          "mean_recall": 0.7555555555555555,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.9502780916157026
        },
        "Semantic": {
          "mean_recall": 0.6777777777777777,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.9342232542717235
        },
        "LLM-Rerank": {
          "mean_recall": 0.5666666666666668,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.8916751285804416
        }
      }
    },
    "L11": {
      "n": 2,
      "approaches": {
        "Hybrid": {
          "mean_recall": 0.7738095238095238,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.8226449141295485
        },
        "BM25": {
          "mean_recall": 0.7619047619047619,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.9199552018097428
        },
        "Semantic": {
          "mean_recall": 0.6071428571428572,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.861456088333812
        },
        "LLM-Rerank": {
          "mean_recall": 0.6785714285714286,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.8746160942694592
        }
      }
    },
    "L12": {
      "n": 1,
      "approaches": {
        "Hybrid": {
          "mean_recall": 0.9,
          "mean_mrr": 0.3333333333333333,
          "mean_ndcg": 0.4590777890428118
        },
        "BM25": {
          "mean_recall": 0.3,
          "mean_mrr": 1.0,
          "mean_ndcg": 0.7448938292196018
        },
        "Semantic": {
          "mean_recall": 0.9,
          "mean_mrr": 0.3333333333333333,
          "mean_ndcg": 0.47742744029197676
        },
        "LLM-Rerank": {
          "mean_recall": 1.0,
          "mean_mrr": 0.25,
          "mean_ndcg": 0.4927598258751643
        }
      }
    }
  },
  "weight_sweep": [
    {
      "kw": 1.0,
      "sw": 0.0,
      "label": "BM25=100% Sem=0%",
      "mean_recall": 0.5248299319727892,
      "pass_rate": 0.6285714285714286
    },
    {
      "kw": 0.9,
      "sw": 0.1,
      "label": "BM25=90% Sem=10%",
      "mean_recall": 0.6525850340136055,
      "pass_rate": 0.8
    },
    {
      "kw": 0.8,
      "sw": 0.2,
      "label": "BM25=80% Sem=20%",
      "mean_recall": 0.6498639455782312,
      "pass_rate": 0.8285714285714286
    },
    {
      "kw": 0.7,
      "sw": 0.3,
      "label": "BM25=70% Sem=30%",
      "mean_recall": 0.6693877551020407,
      "pass_rate": 0.8571428571428571
    },
    {
      "kw": 0.6,
      "sw": 0.4,
      "label": "BM25=60% Sem=40%",
      "mean_recall": 0.7025850340136055,
      "pass_rate": 0.9142857142857143
    },
    {
      "kw": 0.5,
      "sw": 0.5,
      "label": "BM25=50% Sem=50%",
      "mean_recall": 0.6878231292517006,
      "pass_rate": 0.8857142857142857
    },
    {
      "kw": 0.4,
      "sw": 0.6,
      "label": "BM25=40% Sem=60%",
      "mean_recall": 0.7166666666666667,
      "pass_rate": 0.9142857142857143
    },
    {
      "kw": 0.3,
      "sw": 0.7,
      "label": "BM25=30% Sem=70%",
      "mean_recall": 0.7214285714285714,
      "pass_rate": 0.9428571428571428
    },
    {
      "kw": 0.2,
      "sw": 0.8,
      "label": "BM25=20% Sem=80%",
      "mean_recall": 0.7230612244897959,
      "pass_rate": 0.9142857142857143
    },
    {
      "kw": 0.1,
      "sw": 0.9,
      "label": "BM25=10% Sem=90%",
      "mean_recall": 0.6992517006802721,
      "pass_rate": 0.8857142857142857
    },
    {
      "kw": 0.0,
      "sw": 1.0,
      "label": "BM25=0% Sem=100%",
      "mean_recall": 0.6808163265306123,
      "pass_rate": 0.8571428571428571
    },
    {
      "kw": null,
      "sw": null,
      "label": "LLM-Rerank",
      "mean_recall": 0.7269387755102041,
      "pass_rate": 0.8857142857142857
    }
  ]
}