{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='text-align:center; padding: 20px 0;'>\n",
    "<h1 style='color:#A31F34; margin-bottom:0;'>MIT AI STUDIO COURSE</h1>\n",
    "<h3 style='color:#8A8B8C; margin-top:5px;'>MAS.664 / MAS.665 / EC.731 / IDS.865</h3>\n",
    "<h2>RAG Workshop: Chat with the Course Website</h2>\n",
    "<p style='font-size:14px; color:#666;'>Lead Professor: Ramesh Raskar, MIT Media Lab</p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "**Workshop Instructor:** [Brandon Sneider](https://linkedin.com/in/brandonsneider) — AI Manager at a defense technology startup, building AI systems for highly regulated environments. Brandon brings practical experience deploying LLMs and RAG pipelines where accuracy, compliance, and auditability are non-negotiable.\n",
    "\n",
    "---\n",
    "\n",
    "In this **30-minute hands-on workshop**, you'll build a **Retrieval-Augmented Generation (RAG)** system that crawls the [AI Studio course website](https://aiforimpact.github.io/), stores it in a vector database, and lets you **chat with the data** — all inside this notebook.\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "Large Language Models (LLMs) are powerful but have two critical limitations:\n",
    "1. **Knowledge cutoff** — they don't know about data after their training date\n",
    "2. **Hallucination** — they confidently generate plausible but incorrect information\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** solves both by retrieving relevant documents from a knowledge base and injecting them into the LLM's prompt as context before generating an answer.\n",
    "\n",
    "```\n",
    "User Question → [Retrieve relevant docs] → [Inject into prompt] → LLM → Grounded Answer\n",
    "```\n",
    "\n",
    "### Key Academic References\n",
    "\n",
    "| Paper | Key Contribution |\n",
    "|-------|------------------|\n",
    "| Lewis et al. (2020) \"[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\" — *NeurIPS 2020* | Introduced RAG: combines retriever + generator, reducing hallucination and improving factual accuracy |\n",
    "| Karpukhin et al. (2020) \"[Dense Passage Retrieval for Open-Domain QA](https://arxiv.org/abs/2004.04906)\" — *EMNLP 2020* | Dense embeddings outperform BM25 keyword search for passage retrieval |\n",
    "| Robertson & Zaragoza (2009) \"[The Probabilistic Relevance Framework: BM25 and Beyond](https://doi.org/10.1561/1500000019)\" | Foundational work on BM25 scoring |\n",
    "| Gao et al. (2024) \"[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)\" | Comprehensive survey: Naive RAG, Advanced RAG, Modular RAG |\n",
    "| Muennighoff et al. (2023) \"[MTEB: Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316)\" | Benchmark for comparing embedding models across 56 datasets |\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "By the end of this workshop, you will be able to:\n",
    "\n",
    "1. **Explain** what RAG is and why it matters for business (reducing hallucination, grounding LLMs in private data)\n",
    "2. **Compare** embedding models and understand when to fine-tune vs use off-the-shelf\n",
    "3. **Build** a document ingestion pipeline that crawls a website, chunks content, and generates embeddings\n",
    "4. **Understand** the difference between BM25 keyword search and semantic vector search, and why hybrid search outperforms either alone\n",
    "5. **Implement** a working RAG chatbot backed by SQLite + sqlite-vec\n",
    "6. **Evaluate** RAG quality and use AI to tune hyperparameters\n",
    "\n",
    "### Tech Stack\n",
    "\n",
    "| Component | Technology | Why |\n",
    "|-----------|-----------|-----|\n",
    "| Database | **SQLite + sqlite-vec** | Zero infrastructure, runs anywhere, scales to millions of vectors |\n",
    "| Keyword Search | **FTS5 (BM25)** | Built into SQLite, great for exact term matching |\n",
    "| Vector Search | **sqlite-vec** | Native cosine similarity, compact binary storage |\n",
    "| Embeddings | **MiniLM-L6-v2** (local) | No API key needed, runs on CPU, 384 dimensions |\n",
    "| LLM | **OpenRouter** | Access to frontier models, OpenAI-compatible API |\n",
    "| Web Scraping | **BeautifulSoup** | Parse the AI Studio website HTML |\n",
    "| Chat UI | **IPython widgets** | Interactive chat right in the notebook |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Setup & Installation (2 min)\n",
    "\n",
    "We install everything we need in one cell. This takes ~30 seconds on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install all dependencies (run once)\n",
    "!pip install -q sqlite-vec fastembed numpy openai requests beautifulsoup4 lxml yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries loaded!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import struct\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import sqlite_vec\n",
    "from bs4 import BeautifulSoup\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "print(\"All libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Initialize the Vector Database (3 min)\n",
    "\n",
    "We use **SQLite** with two extensions:\n",
    "- **FTS5** — Full-Text Search for BM25 keyword matching (built into SQLite)\n",
    "- **sqlite-vec** — Vector similarity search using cosine distance\n",
    "\n",
    "This gives us a **hybrid search** system in a single file — no external database servers needed.\n",
    "\n",
    "> **Why hybrid?** Karpukhin et al. (2020) showed dense retrieval beats BM25 for semantic queries,\n",
    "> but BM25 still wins for exact name/term lookups. Combining both gets the best of each.\n",
    "\n",
    "| Query Example | BM25 (keyword) | Semantic (vector) | Best Approach |\n",
    "|--------------|:-:|:-:|---|\n",
    "| \"Ramesh Raskar\" | Exact match | May miss | **BM25** |\n",
    "| \"Who teaches AI ethics?\" | No exact terms | Understands meaning | **Semantic** |\n",
    "| \"MIT Media Lab AI course\" | Partial match | Related concepts | **Hybrid** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created at: ai_studio_rag.db\n",
      "sqlite-vec loaded — ready for vector search!\n"
     ]
    }
   ],
   "source": [
    "DATABASE_PATH = Path(\"ai_studio_rag.db\")\n",
    "\n",
    "def init_database(db_path: Path) -> sqlite3.Connection:\n",
    "    \"\"\"Create database with embeddings table, FTS5 for BM25, and vec0 for vectors.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    conn.row_factory = sqlite3.Row\n",
    "\n",
    "    # Load sqlite-vec extension\n",
    "    conn.enable_load_extension(True)\n",
    "    sqlite_vec.load(conn)\n",
    "    conn.enable_load_extension(False)\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Metadata table — stores the actual content and source info\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS documents (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            url TEXT NOT NULL,\n",
    "            page_title TEXT,\n",
    "            section_title TEXT,\n",
    "            content TEXT NOT NULL,\n",
    "            content_type TEXT DEFAULT 'text',\n",
    "            metadata TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # Vector table — 384 dimensions for MiniLM-L6-v2 embeddings\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE VIRTUAL TABLE IF NOT EXISTS vec_documents USING vec0(\n",
    "            embedding float[384] distance_metric=cosine\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # FTS5 table — for BM25 keyword search\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(\n",
    "            content,\n",
    "            page_title,\n",
    "            section_title,\n",
    "            content='documents',\n",
    "            content_rowid='id'\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # Triggers to keep FTS5 in sync automatically\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TRIGGER IF NOT EXISTS docs_ai AFTER INSERT ON documents BEGIN\n",
    "            INSERT INTO documents_fts(rowid, content, page_title, section_title)\n",
    "            VALUES (new.id, new.content, new.page_title, new.section_title);\n",
    "        END\n",
    "    \"\"\")\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TRIGGER IF NOT EXISTS docs_ad AFTER DELETE ON documents BEGIN\n",
    "            INSERT INTO documents_fts(documents_fts, rowid, content, page_title, section_title)\n",
    "            VALUES ('delete', old.id, old.content, old.page_title, old.section_title);\n",
    "        END\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "# Initialize!\n",
    "db = init_database(DATABASE_PATH)\n",
    "print(f\"Database created at: {DATABASE_PATH}\")\n",
    "print(\"sqlite-vec loaded — ready for vector search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Crawl the AI Studio Website & Video Transcripts (5 min)\n",
    "\n",
    "We'll scrape every page of [aiforimpact.github.io](https://aiforimpact.github.io/) including:\n",
    "- Course overview and structure\n",
    "- Speaker & mentor bios (40+ people across 7 semesters)\n",
    "- Schedule and registration info\n",
    "- Past semester archives (Fall 2023 → Spring 2026)\n",
    "- **YouTube video transcripts** (Demo Days, NANDA talks, lectures — ~97K words!)\n",
    "\n",
    "### The RAG Ingestion Pipeline\n",
    "\n",
    "```\n",
    "Website HTML → Crawl & Clean → Chunk by Section ─┐\n",
    "                                                   ├→ Embed → Store in SQLite\n",
    "YouTube Videos → Download Captions → Chunk ────────┘\n",
    "```\n",
    "\n",
    "**Chunking strategy**: We split HTML by sections (headers) and transcripts by ~500-word windows.\n",
    "This matters — Gao et al. (2024) show that chunk size and boundary selection significantly impact retrieval quality.\n",
    "\n",
    "**Why transcripts matter**: The course has 9 YouTube videos (Demo Days, Raskar's NANDA talks) with\n",
    "auto-generated captions. Without transcripts, questions like \"What projects were at Demo Day?\"\n",
    "would return nothing — the answers only exist in what speakers *said*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Website Crawler ──────────────────────────────────────────────────────────\n\nSITE_PAGES = [\n    \"https://aiforimpact.github.io/\",\n    \"https://aiforimpact.github.io/spring26.html\",\n    \"https://aiforimpact.github.io/fall25.html\",\n    \"https://aiforimpact.github.io/spring25.html\",\n    \"https://aiforimpact.github.io/fall24.html\",\n    \"https://aiforimpact.github.io/spring24.html\",\n    \"https://aiforimpact.github.io/fall23.html\",\n]\n\n# Labels that appear before person names in HTML cards\nBIO_LABELS = {\"lead professor:\", \"co-instructor:\", \"instructor:\", \"course ta:\",\n              \"ta:\", \"course instructor:\", \"professor:\", \"guest speaker:\",\n              \"speaker:\", \"mentor:\", \"judge:\", \"panelist:\", \"moderator:\"}\n\n\ndef get_youtube_title(video_id: str) -> tuple[str, str]:\n    \"\"\"Get real YouTube title + author via oEmbed (no API key needed).\"\"\"\n    try:\n        resp = requests.get(\n            f\"https://www.youtube.com/oembed?url=https://www.youtube.com/watch?v={video_id}&format=json\",\n            timeout=10)\n        if resp.status_code == 200:\n            data = resp.json()\n            return data.get(\"title\", \"\"), data.get(\"author_name\", \"\")\n    except Exception:\n        pass\n    return \"\", \"\"\n\n\ndef crawl_page(url: str) -> dict:\n    \"\"\"Fetch and parse a single page, extracting structured content.\"\"\"\n    resp = requests.get(url, timeout=15)\n    resp.raise_for_status()\n    soup = BeautifulSoup(resp.text, \"lxml\")\n\n    title = soup.title.string.strip() if soup.title else url.split('/')[-1]\n\n    for tag in soup([\"script\", \"style\", \"nav\"]):\n        tag.decompose()\n\n    chunks = []\n    seen_names = set()\n\n    # ── Extract speaker/mentor bios (with fixed name parsing + dedup) ──\n    for card in soup.select(\".card, .speaker-card, .col-md-3, .col-lg-3\"):\n        img = card.find(\"img\")\n        text_parts = [t.strip() for t in card.stripped_strings]\n        if not text_parts or len(text_parts) < 2:\n            continue\n\n        # Skip label prefixes like \"Lead Professor:\"\n        name_idx = 0\n        if text_parts[0].lower().rstrip(\":\") + \":\" in BIO_LABELS or text_parts[0].lower() in BIO_LABELS:\n            name_idx = 1\n            if len(text_parts) < 3:\n                continue\n\n        name = text_parts[name_idx]\n        label = text_parts[0] if name_idx > 0 else \"\"\n        role = \" \".join(text_parts[name_idx + 1:])\n        img_url = urljoin(url, img[\"src\"]) if img and img.get(\"src\") else None\n\n        # Deduplicate within page\n        name_key = name.lower().strip()\n        if name_key in seen_names or len(name) < 2:\n            continue\n        seen_names.add(name_key)\n\n        bio_text = f\"Speaker/Mentor: {name}.\"\n        if label:\n            bio_text += f\" Position: {label.rstrip(':').strip()}.\"\n        bio_text += f\" Role: {role}.\"\n        if img_url:\n            bio_text += f\" Photo: {img_url}\"\n\n        chunks.append({\n            \"content\": bio_text,\n            \"section_title\": f\"Bio: {name}\",\n            \"content_type\": \"bio\",\n            \"metadata\": {\"name\": name, \"role\": role, \"label\": label, \"image_url\": img_url}\n        })\n\n    # ── Extract YouTube videos (with real titles via oEmbed + dedup) ──\n    seen_videos = set()\n    for iframe in soup.find_all(\"iframe\"):\n        src = iframe.get(\"src\", \"\")\n        if \"youtube\" not in src and \"youtu.be\" not in src:\n            continue\n        match = re.search(r'(?:embed/|v=|youtu\\.be/)([a-zA-Z0-9_-]{11})', src)\n        if not match:\n            continue\n        vid_id = match.group(1)\n        if vid_id in seen_videos:\n            continue\n        seen_videos.add(vid_id)\n\n        real_title, author = get_youtube_title(vid_id)\n        video_title = real_title or iframe.get(\"title\", \"Course Video\")\n\n        content = f\"Video: {video_title}.\"\n        if author:\n            content += f\" By: {author}.\"\n        content += f\" YouTube URL: https://www.youtube.com/watch?v={vid_id}.\"\n\n        chunks.append({\n            \"content\": content,\n            \"section_title\": f\"Video: {video_title[:60]}\",\n            \"content_type\": \"video\",\n            \"metadata\": {\"video_id\": vid_id, \"title\": video_title, \"author\": author}\n        })\n\n    # ── Extract images (skip bio photos to avoid duplication) ──\n    bio_img_urls = {\n        c[\"metadata\"][\"image_url\"] for c in chunks\n        if c[\"content_type\"] == \"bio\" and c[\"metadata\"].get(\"image_url\")\n    }\n    for img in soup.find_all(\"img\"):\n        src = img.get(\"src\", \"\")\n        alt = img.get(\"alt\", \"\")\n        if not src or src.startswith(\"data:\") or not alt:\n            continue\n        img_url = urljoin(url, src)\n        if img_url in bio_img_urls:\n            continue\n        parent_text = img.parent.get_text(strip=True)[:200] if img.parent else \"\"\n        chunks.append({\n            \"content\": f\"Image: {alt}. URL: {img_url}. Context: {parent_text}\",\n            \"section_title\": f\"Image: {alt[:50]}\",\n            \"content_type\": \"image\",\n            \"metadata\": {\"image_url\": img_url, \"alt_text\": alt}\n        })\n\n    # ── Extract main text content by sections ──\n    main = soup.find(\"main\") or soup.find(\"body\")\n    if main:\n        current_section = \"Overview\"\n        current_text = []\n\n        for element in main.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"p\", \"li\", \"td\", \"th\"]):\n            if element.name in [\"h1\", \"h2\", \"h3\"]:\n                if current_text:\n                    text = \"\\n\".join(current_text).strip()\n                    if len(text) > 30:\n                        chunks.append({\n                            \"content\": f\"{current_section}\\n\\n{text}\",\n                            \"section_title\": current_section,\n                            \"content_type\": \"text\",\n                            \"metadata\": {}\n                        })\n                current_section = element.get_text(strip=True)\n                current_text = []\n            else:\n                text = element.get_text(strip=True)\n                if text and len(text) > 5:\n                    current_text.append(text)\n\n        if current_text:\n            text = \"\\n\".join(current_text).strip()\n            if len(text) > 30:\n                chunks.append({\n                    \"content\": f\"{current_section}\\n\\n{text}\",\n                    \"section_title\": current_section,\n                    \"content_type\": \"text\",\n                    \"metadata\": {}\n                })\n\n    return {\"url\": url, \"title\": title, \"chunks\": chunks}\n\n\ndef download_transcript(video_id: str) -> str:\n    \"\"\"Download YouTube auto-captions as plain text via yt-dlp.\"\"\"\n    import tempfile\n\n    import yt_dlp\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        out_path = os.path.join(tmpdir, video_id)\n        ydl_opts = {\n            \"skip_download\": True,\n            \"writeautomaticsub\": True,\n            \"subtitleslangs\": [\"en\"],\n            \"subtitlesformat\": \"json3\",\n            \"outtmpl\": out_path,\n            \"quiet\": True,\n            \"no_warnings\": True,\n        }\n        try:\n            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n                ydl.download([f\"https://www.youtube.com/watch?v={video_id}\"])\n\n            # Parse the json3 file\n            json3_path = f\"{out_path}.en.json3\"\n            if not os.path.exists(json3_path):\n                return \"\"\n\n            with open(json3_path) as f:\n                data = json.load(f)\n\n            texts = []\n            for event in data.get(\"events\", []):\n                segs = event.get(\"segs\", [])\n                text = \"\".join(s.get(\"utf8\", \"\") for s in segs).strip().replace(\"\\n\", \" \")\n                if text and text not in texts[-1:]:\n                    texts.append(text)\n            return \" \".join(texts)\n        except Exception as e:\n            print(f\"    Transcript error for {video_id}: {e}\")\n            return \"\"\n\n\ndef ingest_transcripts(all_chunks: list[dict]) -> list[dict]:\n    \"\"\"Download transcripts for all video chunks and add as transcript chunks.\"\"\"\n    transcript_chunks = []\n    seen_ids = set()\n    videos = []\n\n    for c in all_chunks:\n        if c[\"content_type\"] == \"video\":\n            meta = c[\"metadata\"]\n            vid_id = meta.get(\"video_id\", \"\")\n            if vid_id and vid_id not in seen_ids:\n                seen_ids.add(vid_id)\n                videos.append((vid_id, meta.get(\"title\", \"Video\")))\n\n    print(f\"\\n  Downloading transcripts for {len(videos)} unique videos...\")\n    for vid_id, title in videos:\n        print(f\"    {title[:50]}...\", end=\" \")\n        text = download_transcript(vid_id)\n        if not text:\n            print(\"(no captions)\")\n            continue\n\n        words = text.split()\n        print(f\"({len(words):,} words)\")\n\n        # Chunk into ~500-word pieces\n        chunk_num = 0\n        for i in range(0, len(words), 450):\n            chunk_words = words[i:i + 500]\n            chunk_text = \" \".join(chunk_words)\n            chunk_num += 1\n            transcript_chunks.append({\n                \"content\": f\"Video transcript: {title}\\nhttps://www.youtube.com/watch?v={vid_id}\\n\\n{chunk_text}\",\n                \"section_title\": f\"Transcript: {title[:45]} (part {chunk_num})\",\n                \"content_type\": \"transcript\",\n                \"url\": f\"https://www.youtube.com/watch?v={vid_id}\",\n                \"page_title\": title,\n                \"metadata\": {\"video_id\": vid_id, \"title\": title, \"chunk\": chunk_num},\n            })\n\n    return transcript_chunks\n\n\ndef crawl_site(pages: list[str]) -> list[dict]:\n    \"\"\"Crawl all pages, fetch video transcripts, return flat list of chunks.\"\"\"\n    all_chunks = []\n    for url in pages:\n        print(f\"  Crawling: {url}\")\n        try:\n            page = crawl_page(url)\n            for chunk in page[\"chunks\"]:\n                chunk[\"url\"] = url\n                chunk[\"page_title\"] = page[\"title\"]\n            all_chunks.extend(page[\"chunks\"])\n            print(f\"    -> {len(page['chunks'])} chunks extracted\")\n        except Exception as e:\n            print(f\"    Error: {e}\")\n\n    # Download and chunk video transcripts\n    transcript_chunks = ingest_transcripts(all_chunks)\n    all_chunks.extend(transcript_chunks)\n\n    return all_chunks\n\n\nprint(\"Crawling the AI Studio website...\\n\")\nall_chunks = crawl_site(SITE_PAGES)\n\n# Summary\ntypes = {}\nfor c in all_chunks:\n    types[c[\"content_type\"]] = types.get(c[\"content_type\"], 0) + 1\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Total chunks extracted: {len(all_chunks)}\")\nfor t, count in sorted(types.items()):\n    print(f\"  {t}: {count}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Embedding Models Deep Dive & Store (5 min)\n",
    "\n",
    "Now we convert each text chunk into a **384-dimensional vector** using the MiniLM-L6-v2 model.\n",
    "\n",
    "**How embeddings work:**\n",
    "- Text → Neural network → Dense vector (array of 384 numbers)\n",
    "- Semantically similar texts produce vectors that are **close together** in vector space\n",
    "- \"Who teaches the course?\" and \"Professor Ramesh Raskar leads the class\" will have similar embeddings, even though they share few words\n",
    "\n",
    "### Why MiniLM-L6-v2? Embedding Model Comparison\n",
    "\n",
    "Choosing an embedding model is one of the most impactful decisions in a RAG system. Here's how the major options compare:\n",
    "\n",
    "| Model | Dims | Size | Cost | Speed | Multilingual | Best For |\n",
    "|-------|------|------|------|-------|-------------|----------|\n",
    "| **all-MiniLM-L6-v2** (ours) | 384 | 80 MB | Free (local) | ~50ms | English only | Prototyping, workshops, CPU |\n",
    "| **BGE-M3** (BAAI) | 1024 | 2.2 GB | Free (local) | ~200ms | 100+ languages | Production multilingual RAG |\n",
    "| **Amazon Titan Embeddings** | 1024 | API | $0.0001/1K tok | ~100ms | 25+ languages | AWS-integrated pipelines |\n",
    "| **OpenAI text-embedding-3-large** | 3072 | API | $0.00013/1K tok | ~80ms | Good | Highest quality, cost-tolerant |\n",
    "| **Cohere embed-v3** | 1024 | API | $0.0001/1K tok | ~90ms | 100+ languages | Search-optimized |\n",
    "\n",
    "**We chose MiniLM-L6-v2 because:**\n",
    "1. **Zero cost** — no API key needed for embeddings, runs entirely on CPU\n",
    "2. **Fast** — smallest model, perfect for a 30-min workshop\n",
    "3. **Good enough** — for English-only <100K docs, quality is comparable to larger models\n",
    "4. **384 dimensions** — smaller vectors = faster search, less storage (vs 1024 or 3072)\n",
    "\n",
    "> **When to upgrade:** For multilingual, use BGE-M3. For max quality, use OpenAI's `text-embedding-3-large`.\n",
    "> See Muennighoff et al. (2023) \"[MTEB: Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316)\" for comprehensive benchmarks.\n",
    "\n",
    "### When and How to Fine-Tune an Embedding Model\n",
    "\n",
    "**When fine-tuning helps** (Wang et al. 2024, \"[Improving Text Embeddings with LLMs](https://arxiv.org/abs/2401.00368)\"):\n",
    "- Your domain has **specialized vocabulary** (legal, medical, finance) that general models don't capture\n",
    "- You have **query-document pairs** showing what users search for vs what they should find\n",
    "- General models score below ~0.7 accuracy on your test queries\n",
    "- Example: A legal RAG where \"consideration\" means contract terms, not \"thinking about\"\n",
    "\n",
    "**When fine-tuning is NOT worth it:**\n",
    "- Your content is general English (like our course website)\n",
    "- You have fewer than ~1,000 labeled query-document pairs\n",
    "- Switching to a larger pre-trained model would solve the problem\n",
    "- You're still iterating on chunking strategy or retrieval logic\n",
    "\n",
    "**How fine-tuning works (conceptual):**\n",
    "```\n",
    "1. Collect pairs:    (query, relevant_document, irrelevant_document)\n",
    "2. Contrastive loss: Push relevant pairs closer, irrelevant pairs apart\n",
    "3. Result:           Model now \"understands\" your domain's similarity\n",
    "```\n",
    "\n",
    "Tools: [Sentence-Transformers fine-tuning](https://www.sbert.net/docs/training/overview.html), or generate synthetic pairs with LLMs (Wang et al. 2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MiniLM-L6-v2 embedding model (384 dimensions, ONNX runtime)...\n",
      "Model loaded!\n",
      "\n",
      "Embedding similarity demo:\n",
      "  Text A: 'Professor Raskar teaches AI at MIT'\n",
      "  Text B: 'The lead instructor of the course is from Media Lab'  (related)\n",
      "  Text C: 'I had pizza for lunch today'  (unrelated)\n",
      "\n",
      "  Similarity A<->B (related):   0.344\n",
      "  Similarity A<->C (unrelated): -0.009\n",
      "  (Higher = more similar. Related texts cluster together!)\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding model (downloads ~80MB on first run)\n",
    "print(\"Loading MiniLM-L6-v2 embedding model (384 dimensions, ONNX runtime)...\")\n",
    "embedding_model = TextEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Model loaded!\\n\")\n",
    "\n",
    "\n",
    "def generate_embedding(text: str) -> list[float]:\n",
    "    \"\"\"Generate a 384-dim embedding for a single text.\"\"\"\n",
    "    return list(embedding_model.embed([text]))[0].tolist()\n",
    "\n",
    "\n",
    "def generate_embeddings_batch(texts: list[str]) -> list[list[float]]:\n",
    "    \"\"\"Batch embed for efficiency.\"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "    return [emb.tolist() for emb in embedding_model.embed(texts)]\n",
    "\n",
    "\n",
    "def serialize_embedding(embedding: list[float]) -> bytes:\n",
    "    \"\"\"Pack embedding as binary for sqlite-vec.\"\"\"\n",
    "    return struct.pack(f'{len(embedding)}f', *embedding)\n",
    "\n",
    "\n",
    "# ── Quick demo: see how embeddings capture meaning ──\n",
    "demo_texts = [\n",
    "    \"Professor Raskar teaches AI at MIT\",\n",
    "    \"The lead instructor of the course is from Media Lab\",\n",
    "    \"I had pizza for lunch today\",\n",
    "]\n",
    "demo_embs = generate_embeddings_batch(demo_texts)\n",
    "\n",
    "print(\"Embedding similarity demo:\")\n",
    "print(f\"  Text A: '{demo_texts[0]}'\")\n",
    "print(f\"  Text B: '{demo_texts[1]}'  (related)\")\n",
    "print(f\"  Text C: '{demo_texts[2]}'  (unrelated)\")\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cosine_sim(a, b): return dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "print(f\"\\n  Similarity A<->B (related):   {cosine_sim(demo_embs[0], demo_embs[1]):.3f}\")\n",
    "print(f\"  Similarity A<->C (unrelated): {cosine_sim(demo_embs[0], demo_embs[2]):.3f}\")\n",
    "print(\"  (Higher = more similar. Related texts cluster together!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and storing all chunks...\n",
      "  Stored 32/748 chunks...\n",
      "  Stored 64/748 chunks...\n",
      "  Stored 96/748 chunks...\n",
      "  Stored 128/748 chunks...\n",
      "  Stored 160/748 chunks...\n",
      "  Stored 192/748 chunks...\n",
      "  Stored 224/748 chunks...\n",
      "  Stored 256/748 chunks...\n",
      "  Stored 288/748 chunks...\n",
      "  Stored 320/748 chunks...\n",
      "  Stored 352/748 chunks...\n",
      "  Stored 384/748 chunks...\n",
      "  Stored 416/748 chunks...\n",
      "  Stored 448/748 chunks...\n",
      "  Stored 480/748 chunks...\n",
      "  Stored 512/748 chunks...\n",
      "  Stored 544/748 chunks...\n",
      "  Stored 576/748 chunks...\n",
      "  Stored 608/748 chunks...\n",
      "  Stored 640/748 chunks...\n",
      "  Stored 672/748 chunks...\n",
      "  Stored 704/748 chunks...\n",
      "  Stored 736/748 chunks...\n",
      "  Stored 748/748 chunks...\n",
      "\n",
      "Done! 748 chunks embedded and stored in ai_studio_rag.db\n",
      "Database size: 4096 KB\n"
     ]
    }
   ],
   "source": [
    "# ── Ingest all chunks into the database ──\n",
    "print(\"Embedding and storing all chunks...\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "total_stored = 0\n",
    "\n",
    "for i in range(0, len(all_chunks), BATCH_SIZE):\n",
    "    batch = all_chunks[i:i + BATCH_SIZE]\n",
    "    texts = [c[\"content\"] for c in batch]\n",
    "    embeddings = generate_embeddings_batch(texts)\n",
    "\n",
    "    cursor = db.cursor()\n",
    "    for chunk, emb in zip(batch, embeddings):\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO documents (url, page_title, section_title, content, content_type, metadata)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            chunk[\"url\"],\n",
    "            chunk[\"page_title\"],\n",
    "            chunk[\"section_title\"],\n",
    "            chunk[\"content\"],\n",
    "            chunk[\"content_type\"],\n",
    "            json.dumps(chunk[\"metadata\"]),\n",
    "        ))\n",
    "        rowid = cursor.lastrowid\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO vec_documents (rowid, embedding) VALUES (?, ?)\n",
    "        \"\"\", (rowid, serialize_embedding(emb)))\n",
    "\n",
    "    db.commit()\n",
    "    total_stored += len(batch)\n",
    "    print(f\"  Stored {total_stored}/{len(all_chunks)} chunks...\")\n",
    "\n",
    "print(f\"\\nDone! {total_stored} chunks embedded and stored in {DATABASE_PATH}\")\n",
    "print(f\"Database size: {DATABASE_PATH.stat().st_size / 1024:.0f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Hybrid Search — BM25 + Semantic (5 min)\n",
    "\n",
    "This is the **retrieval** core of RAG. We implement two search strategies and combine them:\n",
    "\n",
    "### BM25 (Keyword Search)\n",
    "- Uses SQLite's **FTS5** extension\n",
    "- Scores based on term frequency / inverse document frequency\n",
    "- Great for exact names, acronyms, specific terms\n",
    "- Based on Robertson & Zaragoza (2009)\n",
    "\n",
    "### Semantic Search (Vector Similarity)\n",
    "- Uses **sqlite-vec**'s native cosine distance\n",
    "- Finds conceptually similar content even with different wording\n",
    "- Based on Karpukhin et al. (2020) Dense Passage Retrieval\n",
    "\n",
    "### Hybrid Fusion\n",
    "We normalize both scores to [0, 1] and combine with configurable weights:\n",
    "\n",
    "```\n",
    "final_score = keyword_weight * BM25_score + semantic_weight * cosine_similarity\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Search Functions ──────────────────────────────────────────────────────────\n\ndef bm25_search(conn, query: str, limit: int = 50) -> dict[int, float]:\n    \"\"\"BM25 keyword search via FTS5. Returns {doc_id: score}.\"\"\"\n    cursor = conn.cursor()\n    safe_query = query.replace('\"', '\"\"')\n    try:\n        cursor.execute(\"\"\"\n            SELECT rowid, bm25(documents_fts) as score\n            FROM documents_fts\n            WHERE documents_fts MATCH ?\n            LIMIT ?\n        \"\"\", (safe_query, limit))\n        return {row[0]: row[1] for row in cursor.fetchall()}\n    except sqlite3.OperationalError:\n        return {}\n\n\ndef semantic_search(conn, query_embedding: list[float], limit: int = 50) -> dict[int, float]:\n    \"\"\"Cosine similarity search via sqlite-vec. Returns {doc_id: distance}.\"\"\"\n    cursor = conn.cursor()\n    cursor.execute(\"\"\"\n        SELECT rowid, distance\n        FROM vec_documents\n        WHERE embedding MATCH ? AND k = ?\n        ORDER BY distance\n    \"\"\", (serialize_embedding(query_embedding), limit))\n    return {row[0]: row[1] for row in cursor.fetchall()}\n\n\ndef normalize_scores(scores: dict[int, float], higher_is_better: bool = True) -> dict[int, float]:\n    \"\"\"Normalize scores to [0, 1].\"\"\"\n    if not scores:\n        return {}\n    vals = list(scores.values())\n    min_v, max_v = min(vals), max(vals)\n    if min_v == max_v:\n        return {k: 1.0 for k in scores}\n    if higher_is_better:\n        return {k: (v - min_v) / (max_v - min_v) for k, v in scores.items()}\n    else:\n        return {k: (max_v - v) / (max_v - min_v) for k, v in scores.items()}\n\n\ndef hybrid_search(\n    conn,\n    query: str,\n    query_embedding: list[float],\n    keyword_weight: float = 0.3,\n    semantic_weight: float = 0.7,\n    top_k: int = 5,\n) -> list[dict]:\n    \"\"\"Combine BM25 + semantic search with weighted fusion and content diversity.\n\n    Diversity: limits any single content_type to at most (top_k - 1) results,\n    ensuring compound queries surface different types (bios, text, transcripts).\n    \"\"\"\n    bm25_raw = bm25_search(conn, query)\n    bm25_norm = normalize_scores(bm25_raw, higher_is_better=False)\n\n    sem_raw = semantic_search(conn, query_embedding)\n    sem_norm = normalize_scores(sem_raw, higher_is_better=False)\n\n    all_ids = set(bm25_norm.keys()) | set(sem_norm.keys())\n    if not all_ids:\n        return []\n\n    cursor = conn.cursor()\n    placeholders = \",\".join(\"?\" * len(all_ids))\n    cursor.execute(f\"\"\"\n        SELECT id, url, page_title, section_title, content, content_type, metadata\n        FROM documents WHERE id IN ({placeholders})\n    \"\"\", list(all_ids))\n    cols = [\"id\",\"url\",\"page_title\",\"section_title\",\"content\",\"content_type\",\"metadata\"]\n    docs = {row[0]: dict(zip(cols, row)) for row in cursor.fetchall()}\n\n    results = []\n    for doc_id in all_ids:\n        bm25_s = bm25_norm.get(doc_id, 0.0)\n        sem_s = sem_norm.get(doc_id, 0.0)\n        final = keyword_weight * bm25_s + semantic_weight * sem_s\n        doc = docs.get(doc_id, {})\n        results.append({**doc, \"bm25_score\": bm25_s, \"semantic_score\": sem_s, \"final_score\": final})\n\n    results.sort(key=lambda x: x[\"final_score\"], reverse=True)\n\n    # ── Content-type diversity: cap any single type at (top_k - 1) ──\n    # This prevents e.g. 5 bios drowning out course description text chunks.\n    max_per_type = max(top_k - 1, 1)\n    diverse = []\n    type_counts = {}\n    for r in results:\n        ct = r.get(\"content_type\", \"text\")\n        type_counts[ct] = type_counts.get(ct, 0) + 1\n        if type_counts[ct] <= max_per_type:\n            diverse.append(r)\n        if len(diverse) >= top_k:\n            break\n\n    return diverse[:top_k]\n\n\n# ── Test it! ──\ntest_queries = [\n    \"Who is the lead professor of the course?\",\n    \"What are the course pillars?\",\n    \"Tell me about the guest speakers from venture capital\",\n]\n\nfor q in test_queries:\n    print(f\"\\n{'='*60}\")\n    print(f\"Query: {q}\")\n    print(f\"{'='*60}\")\n    emb = generate_embedding(q)\n    results = hybrid_search(db, q, emb, top_k=3)\n    for i, r in enumerate(results, 1):\n        print(f\"  {i}. [{r.get('content_type','?')}] Score: {r['final_score']:.3f} \"\n              f\"(BM25: {r['bm25_score']:.2f}, Semantic: {r['semantic_score']:.2f})\")\n        content_preview = r.get('content', '')[:120].replace('\\n', ' ')\n        print(f\"     {content_preview}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Connect the LLM for Generation (3 min)\n",
    "\n",
    "Now we add the **G** in RA**G** — we take retrieved context and use an LLM to generate a grounded answer.\n",
    "\n",
    "### How to Get an API Key\n",
    "\n",
    "We support **two providers** — use whichever you already have:\n",
    "\n",
    "**Option A: OpenRouter (recommended for this workshop)**\n",
    "1. Go to [openrouter.ai/keys](https://openrouter.ai/keys)\n",
    "2. Sign in with Google/GitHub (free, no credit card)\n",
    "3. Click **\"Create Key\"** — copy the key (starts with `sk-or-`)\n",
    "4. In Colab: click the **key icon** in the left sidebar → add secret named `OPENROUTER_API_KEY`\n",
    "\n",
    "*OpenRouter gives you access to many models (Gemini, Llama, Mistral) through one API.*\n",
    "\n",
    "**Option B: OpenAI API key**\n",
    "1. Go to [platform.openai.com/api-keys](https://platform.openai.com/api-keys)\n",
    "2. Create a new key (starts with `sk-proj-` or `sk-`)\n",
    "3. In Colab: add secret named `OPENAI_API_KEY`\n",
    "\n",
    "*Uses GPT-4o-mini by default. Requires billing enabled on your OpenAI account.*\n",
    "\n",
    "> **Note on Anthropic/Claude keys:** Claude's API uses a different message format (not OpenAI-compatible).\n",
    "> Keys starting with `sk-ant-` will **not work** here. Use OpenRouter instead — it routes to Claude\n",
    "> models via the OpenAI-compatible format if you want Claude.\n",
    "\n",
    "### The RAG Prompt Pattern\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant for the MIT AI Studio course.\n",
    "        Answer ONLY using the provided context.\n",
    "\n",
    "User:   Context: [retrieved documents]\n",
    "        Question: [user's question]\n",
    "```\n",
    "\n",
    "This is the key insight from Lewis et al. (2020): by conditioning generation on retrieved evidence, the model produces **factual, verifiable answers** rather than hallucinating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenRouter key detected -> using model: google/gemini-2.0-flash-001\n",
      "\n",
      "LLM client connected! Provider: openrouter\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# ── Auto-detect API key type and configure provider ──\n",
    "# Supports: OpenRouter (sk-or-), OpenAI (sk-proj- / sk-), rejects Anthropic (sk-ant-)\n",
    "\n",
    "api_key = None\n",
    "provider = None\n",
    "\n",
    "# Try Colab Secrets first (key icon in left sidebar)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    for secret_name in [\"OPENROUTER_API_KEY\", \"OPENAI_API_KEY\"]:\n",
    "        try:\n",
    "            key = userdata.get(secret_name)\n",
    "            if key:\n",
    "                api_key = key\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "except ImportError:\n",
    "    # Not running in Colab — check environment variables\n",
    "    import os\n",
    "    api_key = os.environ.get(\"OPENROUTER_API_KEY\") or os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Fall back to manual input\n",
    "if not api_key:\n",
    "    api_key = input(\n",
    "        \"Enter your API key (OpenRouter or OpenAI):\\n\"\n",
    "        \"  OpenRouter: https://openrouter.ai/keys (free, recommended)\\n\"\n",
    "        \"  OpenAI:     https://platform.openai.com/api-keys\\n> \"\n",
    "    ).strip()\n",
    "\n",
    "# ── Detect provider from key prefix ──\n",
    "if api_key.startswith(\"sk-ant-\"):\n",
    "    raise ValueError(\n",
    "        \"\\n\\nAnthropic/Claude key detected (sk-ant-...).\\n\"\n",
    "        \"Claude uses a different API message format and is NOT compatible with the OpenAI SDK.\\n\\n\"\n",
    "        \"Options:\\n\"\n",
    "        \"  1. Get a free OpenRouter key at https://openrouter.ai/keys\\n\"\n",
    "        \"     (OpenRouter can route to Claude models via OpenAI-compatible format)\\n\"\n",
    "        \"  2. Use an OpenAI key from https://platform.openai.com/api-keys\\n\"\n",
    "    )\n",
    "elif api_key.startswith(\"sk-or-\"):\n",
    "    provider = \"openrouter\"\n",
    "    base_url = \"https://openrouter.ai/api/v1\"\n",
    "    LLM_MODEL = \"google/gemini-2.0-flash-001\"\n",
    "    print(f\"OpenRouter key detected -> using model: {LLM_MODEL}\")\n",
    "elif api_key.startswith(\"sk-proj-\") or api_key.startswith(\"sk-\"):\n",
    "    provider = \"openai\"\n",
    "    base_url = \"https://api.openai.com/v1\"\n",
    "    LLM_MODEL = \"gpt-4o-mini\"\n",
    "    print(f\"OpenAI key detected -> using model: {LLM_MODEL}\")\n",
    "else:\n",
    "    # Unknown prefix — assume OpenRouter (most permissive)\n",
    "    provider = \"openrouter\"\n",
    "    base_url = \"https://openrouter.ai/api/v1\"\n",
    "    LLM_MODEL = \"google/gemini-2.0-flash-001\"\n",
    "    print(f\"Unknown key format — defaulting to OpenRouter with model: {LLM_MODEL}\")\n",
    "\n",
    "client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "print(f\"\\nLLM client connected! Provider: {provider}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who are the lead instructors and what is the course about?\n",
      "\n",
      "A: Based on the context, Ramesh Raskar (Lead Professor, MIT Media Lab) is a lead instructor for the AI Studio course. The context does not provide information about what the course is about.\n",
      "\n",
      "*   **Ramesh Raskar:** Lead Professor, MIT Media Lab. Photo: [https://aiforimpact.github.io/assets/img/speakers/ramesh.png](https://aiforimpact.github.io/assets/img/speakers/ramesh.png)\n",
      "\n",
      "Sources used: 5 documents\n",
      "  - [bio] Bio: David Shrier (score: 0.70)\n",
      "  - [bio] Bio: Nikolay Vyahhi (score: 0.63)\n",
      "  - [bio] Bio: Nikolay Vyahhi (score: 0.63)\n",
      "  - [bio] Bio: Ramesh Raskar (score: 0.61)\n",
      "  - [bio] Bio: John Werner (score: 0.56)\n"
     ]
    }
   ],
   "source": [
    "# ── RAG Answer Function ───────────────────────────────────────────────────────\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant for the MIT AI Studio course\n",
    "(MAS.664 / MAS.665 / EC.731 / IDS.865), taught at MIT Media Lab by Professor Ramesh Raskar.\n",
    "\n",
    "Rules:\n",
    "- Answer ONLY using the provided context. Do not use outside knowledge.\n",
    "- If the context mentions images or videos, include the URLs so the user can view them.\n",
    "- If the context doesn't contain enough information, say so clearly.\n",
    "- Be concise but thorough. Use bullet points for lists.\n",
    "- When mentioning people, include their role/affiliation if available.\"\"\"\n",
    "\n",
    "\n",
    "def format_context(results: list[dict], max_chars: int = 4000) -> str:\n",
    "    \"\"\"Format retrieved documents into context for the LLM.\"\"\"\n",
    "    if not results:\n",
    "        return \"No relevant documents found.\"\n",
    "    parts = []\n",
    "    chars = 0\n",
    "    for i, r in enumerate(results, 1):\n",
    "        entry = f\"[Source {i}: {r.get('page_title', '?')} > {r.get('section_title', '?')}]\\n{r.get('content', '')}\"\n",
    "        if chars + len(entry) > max_chars:\n",
    "            break\n",
    "        parts.append(entry)\n",
    "        chars += len(entry)\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "def rag_answer(question: str, conn=db, top_k: int = 5) -> tuple[str, list[dict]]:\n",
    "    \"\"\"Full RAG pipeline: retrieve -> format -> generate.\"\"\"\n",
    "    query_emb = generate_embedding(question)\n",
    "    results = hybrid_search(conn, question, query_emb, top_k=top_k)\n",
    "    context = format_context(results)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        max_tokens=800,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"},\n",
    "        ],\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    return answer, results\n",
    "\n",
    "\n",
    "# ── Quick test ──\n",
    "question = \"Who are the lead instructors and what is the course about?\"\n",
    "answer, sources = rag_answer(question)\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(f\"A: {answer}\\n\")\n",
    "print(f\"Sources used: {len(sources)} documents\")\n",
    "for s in sources:\n",
    "    print(f\"  - [{s.get('content_type','')}] {s.get('section_title', '')[:60]} (score: {s['final_score']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Interactive Chat Interface (5 min)\n",
    "\n",
    "Now let's build an interactive chat — right here in the notebook!\n",
    "\n",
    "Try asking:\n",
    "- \"Who are the venture capital speakers?\"\n",
    "- \"What's the difference between Spring 2025 and Fall 2025?\"\n",
    "- \"What are the course pillars?\"\n",
    "- \"Tell me about the demo day\"\n",
    "- \"Are there any videos I can watch?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── In-Notebook Chat UI ───────────────────────────────────────────────────────\n\nimport ipywidgets as widgets\nfrom IPython.display import HTML, clear_output, display\n\nchat_output = widgets.Output(layout=widgets.Layout(\n    width='100%', min_height='300px', max_height='500px',\n    overflow_y='auto', border='1px solid #444', padding='10px',\n))\ninput_box = widgets.Text(\n    placeholder='Ask anything about the MIT AI Studio course...',\n    layout=widgets.Layout(width='85%'),\n)\nsend_btn = widgets.Button(description='Send', button_style='primary',\n                          layout=widgets.Layout(width='14%'))\nshow_sources = widgets.Checkbox(value=False, description='Show sources', indent=False)\n\nchat_history = []\n\n\ndef _extract_source_images(sources: list[dict], max_images: int = 3) -> str:\n    \"\"\"Extract speaker photos from bio sources and return as HTML img tags.\"\"\"\n    if not sources:\n        return \"\"\n    imgs = []\n    seen_urls = set()\n    for s in sources:\n        meta = s.get(\"metadata\", \"\")\n        if isinstance(meta, str):\n            try:\n                meta = json.loads(meta)\n            except (json.JSONDecodeError, TypeError):\n                meta = {}\n        img_url = meta.get(\"image_url\", \"\")\n        if img_url and img_url not in seen_urls and s.get(\"content_type\") == \"bio\":\n            seen_urls.add(img_url)\n            name = meta.get(\"name\", \"Speaker\")\n            imgs.append(\n                f\"<img src='{img_url}' alt='{name}' title='{name}' \"\n                f\"style='width:60px;height:60px;border-radius:50%;object-fit:cover;\"\n                f\"margin:2px 4px;border:2px solid #555;'>\"\n            )\n        if len(imgs) >= max_images:\n            break\n    if imgs:\n        return f\"<div style='margin:6px 0;'>{''.join(imgs)}</div>\"\n    return \"\"\n\n\ndef render_chat():\n    with chat_output:\n        clear_output()\n        for msg in chat_history:\n            if msg[\"role\"] == \"user\":\n                display(HTML(f\"<div style='margin:8px 0;padding:10px 14px;background:#1a3a5c;\"\n                    f\"border-radius:12px;color:white;max-width:80%;margin-left:auto;\"\n                    f\"text-align:right;font-size:14px;'><b>You:</b> {msg['content']}</div>\"))\n            else:\n                # Render speaker photos from bio sources\n                images_html = _extract_source_images(msg.get(\"sources\", []))\n                sources_html = \"\"\n                if show_sources.value and msg.get(\"sources\"):\n                    src_items = \"\".join(\n                        f\"<br>- [{s.get('content_type','')}] {s.get('section_title','')[:50]} (score: {s['final_score']:.2f})\"\n                        for s in msg[\"sources\"]\n                    )\n                    sources_html = f\"<br><details><summary><small>Sources</small></summary><small>{src_items}</small></details>\"\n                display(HTML(f\"<div style='margin:8px 0;padding:10px 14px;background:#2d2d2d;\"\n                    f\"border-radius:12px;color:#e0e0e0;max-width:90%;font-size:14px;\"\n                    f\"line-height:1.5;'><b>AI Studio Assistant:</b><br>{msg['content']}\"\n                    f\"{images_html}{sources_html}</div>\"))\n\ndef on_send(btn):\n    question = input_box.value.strip()\n    if not question:\n        return\n    input_box.value = \"\"\n    chat_history.append({\"role\": \"user\", \"content\": question})\n    render_chat()\n    with chat_output:\n        display(HTML(\"<div style='color:#888;padding:5px;font-style:italic;'>Searching & generating...</div>\"))\n    try:\n        answer, sources = rag_answer(question)\n        chat_history.append({\"role\": \"assistant\", \"content\": answer, \"sources\": sources})\n    except Exception as e:\n        chat_history.append({\"role\": \"assistant\", \"content\": f\"Error: {e}\", \"sources\": []})\n    render_chat()\n\nsend_btn.on_click(on_send)\ninput_box.on_submit(lambda _: on_send(None))\nshow_sources.observe(lambda _: render_chat(), names='value')\n\nheader = widgets.HTML(\"<h3 style='margin:0;padding:8px 0;'>Chat with the AI Studio Course</h3>\")\ndisplay(widgets.VBox([header, chat_output, widgets.HBox([input_box, send_btn]), widgets.HBox([show_sources])]))\n\nchat_history.append({\n    \"role\": \"assistant\",\n    \"content\": \"Welcome! I've ingested the entire <b>MIT AI Studio course website</b> \"\n               \"(7 pages, all semesters, 40+ speaker bios, videos, and images). \"\n               \"Ask me anything!<br><br>\"\n               \"<b>Try:</b> 'Who are the VC speakers?' or 'What is the course about?' \"\n               \"or 'How do I register for Spring 2026?'\",\n    \"sources\": []\n})\nrender_chat()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Understanding What Just Happened — RAG Recap\n",
    "\n",
    "Let's trace what happens when you type a question in the chat:\n",
    "\n",
    "```\n",
    " 1. EMBED        Your question -> 384-dim vector\n",
    "                 \"Who teaches AI ethics?\" -> [0.03, -0.12, ...]\n",
    "\n",
    " 2. RETRIEVE     Hybrid search over the database\n",
    "                 BM25:     keyword match on \"teaches\", \"AI\"\n",
    "                 Semantic: cosine similarity to find related\n",
    "                 -> Top 5 most relevant chunks\n",
    "\n",
    " 3. AUGMENT      Inject retrieved chunks into the LLM prompt\n",
    "                 \"Context: [Source 1: ...] [Source 2: ...]\"\n",
    "\n",
    " 4. GENERATE     LLM produces answer grounded in evidence\n",
    "                 -> Factual, verifiable, no hallucination\n",
    "```\n",
    "\n",
    "### Why RAG Matters for Business (MBA Perspective)\n",
    "\n",
    "| Use Case | Without RAG | With RAG |\n",
    "|----------|-------------|----------|\n",
    "| Customer support chatbot | Generic answers, can't access your docs | Answers from your actual knowledge base |\n",
    "| Legal document review | May hallucinate case law | Grounded in your actual contracts |\n",
    "| Internal company wiki Q&A | Outdated training data | Always current with your docs |\n",
    "| Due diligence research | May confuse companies | Grounded in actual filings |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **RAG = Retrieve + Augment + Generate** — give the LLM the right context, get grounded answers\n",
    "2. **Hybrid search > either alone** — combine keyword (BM25) and semantic (vector) for best results\n",
    "3. **Embeddings are the bridge** — they convert text to numbers so we can measure similarity\n",
    "4. **Model choice matters** — MiniLM for prototypes, BGE-M3/OpenAI for production\n",
    "5. **The entire system runs locally** — embeddings are local, only the LLM call needs an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Compare Search Strategies\n",
    "Run the cell below to see how BM25-only, semantic-only, and hybrid search differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Compare search strategies for different query types\n",
    "\n",
    "test_cases = [\n",
    "    (\"Ramesh Raskar\", \"Exact name lookup - BM25 should excel\"),\n",
    "    (\"venture capital investors in AI\", \"Conceptual query - semantic should excel\"),\n",
    "    (\"MIT Media Lab course spring 2026\", \"Mix of specific + conceptual - hybrid wins\"),\n",
    "]\n",
    "\n",
    "for query, description in test_cases:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Query: \\\"{query}\\\"\")\n",
    "    print(f\"Expected: {description}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    emb = generate_embedding(query)\n",
    "\n",
    "    r_bm25 = hybrid_search(db, query, emb, keyword_weight=1.0, semantic_weight=0.0, top_k=3)\n",
    "    r_sem = hybrid_search(db, query, emb, keyword_weight=0.0, semantic_weight=1.0, top_k=3)\n",
    "    r_hyb = hybrid_search(db, query, emb, keyword_weight=0.3, semantic_weight=0.7, top_k=3)\n",
    "\n",
    "    for label, results in [(\"BM25 Only\", r_bm25), (\"Semantic Only\", r_sem), (\"Hybrid\", r_hyb)]:\n",
    "        print(f\"\\n  {label}:\")\n",
    "        if not results:\n",
    "            print(\"    (no results)\")\n",
    "        for r in results[:2]:\n",
    "            print(f\"    - {r.get('section_title', '?')[:50]} (score: {r['final_score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Add Your Own Content\n",
    "Add a new document to the knowledge base and query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Add your own content to the RAG system\n",
    "\n",
    "my_doc = \"\"\"\n",
    "Replace this with your own text! For example, paste your startup idea,\n",
    "a paragraph from a paper you're reading, or notes from a lecture.\n",
    "\"\"\"\n",
    "\n",
    "emb = generate_embedding(my_doc)\n",
    "cursor = db.cursor()\n",
    "cursor.execute(\"\"\"\n",
    "    INSERT INTO documents (url, page_title, section_title, content, content_type, metadata)\n",
    "    VALUES (?, ?, ?, ?, ?, ?)\n",
    "\"\"\", (\"user://custom\", \"My Document\", \"Custom Content\", my_doc, \"text\", \"{}\"))\n",
    "rowid = cursor.lastrowid\n",
    "cursor.execute(\"INSERT INTO vec_documents (rowid, embedding) VALUES (?, ?)\",\n",
    "               (rowid, serialize_embedding(emb)))\n",
    "db.commit()\n",
    "print(f\"Added your document (id={rowid}). Now try asking about it in the chat above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Let AI Tune Your RAG Hyperparameters\n",
    "\n",
    "Our RAG system has several \"knobs\" (hyperparameters) that affect quality. Right now they're set to reasonable defaults — but are they optimal for *this* dataset?\n",
    "\n",
    "**The hyperparameters we can tune:**\n",
    "- `keyword_weight` / `semantic_weight` — balance between BM25 and vector search\n",
    "- `top_k` — how many documents to retrieve\n",
    "\n",
    "Below, we first test with **untuned defaults**, then ask the LLM to analyze results and suggest better values. This is a real technique used in production RAG systems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Exercise 3: Untuned vs AI-Tuned Hyperparameters ─────────────────────────\n",
    "\n",
    "EVAL_QUERIES = [\n",
    "    {\"query\": \"Who is the lead professor?\",\n",
    "     \"expected_keywords\": [\"ramesh\", \"raskar\", \"media lab\"]},\n",
    "    {\"query\": \"What are the three course pillars?\",\n",
    "     \"expected_keywords\": [\"innovation\", \"human centered\", \"technical\"]},\n",
    "    {\"query\": \"Which venture capitalists are speakers?\",\n",
    "     \"expected_keywords\": [\"khosla\", \"lux\", \"pillar\"]},\n",
    "    {\"query\": \"How do I register for Spring 2026?\",\n",
    "     \"expected_keywords\": [\"questionnaire\", \"register\", \"step\"]},\n",
    "    {\"query\": \"What happens at demo day?\",\n",
    "     \"expected_keywords\": [\"demo\", \"presentations\", \"investors\"]},\n",
    "]\n",
    "\n",
    "def score_results(results: list[dict], expected_keywords: list[str]) -> float:\n",
    "    \"\"\"Score retrieval: what fraction of expected keywords appear in top results?\"\"\"\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    all_content = \" \".join(r.get(\"content\", \"\") for r in results).lower()\n",
    "    hits = sum(1 for kw in expected_keywords if kw.lower() in all_content)\n",
    "    return hits / len(expected_keywords)\n",
    "\n",
    "def evaluate_config(kw_w, sem_w, top_k):\n",
    "    \"\"\"Run all eval queries and return average retrieval score.\"\"\"\n",
    "    scores = []\n",
    "    for eq in EVAL_QUERIES:\n",
    "        emb = generate_embedding(eq[\"query\"])\n",
    "        results = hybrid_search(db, eq[\"query\"], emb,\n",
    "                               keyword_weight=kw_w, semantic_weight=sem_w, top_k=top_k)\n",
    "        scores.append(score_results(results, eq[\"expected_keywords\"]))\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "# ── Step 1: Test with UNTUNED defaults ──\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Testing with UNTUNED defaults\")\n",
    "print(\"  keyword_weight=0.3, semantic_weight=0.7, top_k=5\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "untuned_score = evaluate_config(0.3, 0.7, 5)\n",
    "print(f\"\\n  Overall retrieval score: {untuned_score:.1%}\\n\")\n",
    "\n",
    "for eq in EVAL_QUERIES:\n",
    "    emb = generate_embedding(eq[\"query\"])\n",
    "    results = hybrid_search(db, eq[\"query\"], emb, keyword_weight=0.3, semantic_weight=0.7, top_k=5)\n",
    "    s = score_results(results, eq[\"expected_keywords\"])\n",
    "    status = \"PASS\" if s >= 0.66 else \"WEAK\" if s > 0 else \"FAIL\"\n",
    "    print(f\"  [{status}] \\\"{eq['query']}\\\" -> {s:.0%} keywords found\")\n",
    "\n",
    "# ── Step 2: Ask AI to suggest better hyperparameters ──\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"STEP 2: Asking AI to analyze and suggest better config...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tuning_prompt = f\"\"\"You are optimizing a RAG system's hyperparameters.\n",
    "The system uses hybrid search combining BM25 keyword search and semantic vector search.\n",
    "\n",
    "Current config: keyword_weight=0.3, semantic_weight=0.7, top_k=5\n",
    "Current retrieval score: {untuned_score:.1%}\n",
    "\n",
    "Eval results:\n",
    "\"\"\"\n",
    "for eq in EVAL_QUERIES:\n",
    "    emb = generate_embedding(eq[\"query\"])\n",
    "    results = hybrid_search(db, eq[\"query\"], emb, keyword_weight=0.3, semantic_weight=0.7, top_k=5)\n",
    "    s = score_results(results, eq[\"expected_keywords\"])\n",
    "    top_sections = [r.get(\"section_title\", \"?\")[:40] for r in results[:3]]\n",
    "    tuning_prompt += (f\"\\n- Query: \\\"{eq['query']}\\\"\\n\"\n",
    "                      f\"  Expected keywords: {eq['expected_keywords']}\\n\"\n",
    "                      f\"  Score: {s:.0%}, Top results: {top_sections}\\n\")\n",
    "\n",
    "tuning_prompt += \"\"\"\\nSuggest improved hyperparameters. Consider:\n",
    "- If name lookups fail, increase keyword_weight\n",
    "- If conceptual queries fail, increase semantic_weight\n",
    "- If not enough context, increase top_k (max 10)\n",
    "\n",
    "Respond ONLY with JSON: {\"keyword_weight\": 0.4, \"semantic_weight\": 0.6, \"top_k\": 7, \"reasoning\": \"brief\"}\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=LLM_MODEL,\n",
    "    max_tokens=200,\n",
    "    messages=[{\"role\": \"user\", \"content\": tuning_prompt}],\n",
    ")\n",
    "ai_response = response.choices[0].message.content.strip()\n",
    "print(f\"\\n  AI suggestion: {ai_response}\\n\")\n",
    "\n",
    "# Parse and test\n",
    "json_match = re.search(r'\\{[^}]+\\}', ai_response)\n",
    "if json_match:\n",
    "    suggestion = json.loads(json_match.group())\n",
    "    new_kw = suggestion.get(\"keyword_weight\", 0.4)\n",
    "    new_sem = suggestion.get(\"semantic_weight\", 0.6)\n",
    "    new_topk = min(suggestion.get(\"top_k\", 7), 10)\n",
    "\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(\"STEP 3: Testing with AI-TUNED config\")\n",
    "    print(f\"  keyword_weight={new_kw}, semantic_weight={new_sem}, top_k={new_topk}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    tuned_score = evaluate_config(new_kw, new_sem, new_topk)\n",
    "    print(f\"\\n  Overall retrieval score: {tuned_score:.1%}\\n\")\n",
    "\n",
    "    for eq in EVAL_QUERIES:\n",
    "        emb = generate_embedding(eq[\"query\"])\n",
    "        results = hybrid_search(db, eq[\"query\"], emb,\n",
    "                               keyword_weight=new_kw, semantic_weight=new_sem, top_k=new_topk)\n",
    "        s = score_results(results, eq[\"expected_keywords\"])\n",
    "        status = \"PASS\" if s >= 0.66 else \"WEAK\" if s > 0 else \"FAIL\"\n",
    "        print(f\"  [{status}] \\\"{eq['query']}\\\" -> {s:.0%} keywords found\")\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"  Untuned score:  {untuned_score:.1%}\")\n",
    "    print(f\"  AI-tuned score: {tuned_score:.1%}\")\n",
    "    delta = tuned_score - untuned_score\n",
    "    if delta > 0:\n",
    "        print(f\"  Improvement:    +{delta:.1%}\")\n",
    "    elif delta == 0:\n",
    "        print(\"  No change (defaults were already good for this dataset)\")\n",
    "    else:\n",
    "        print(f\"  Change:         {delta:.1%} (AI suggestion didn't help here)\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "else:\n",
    "    print(\"  Could not parse AI suggestion. Try running the cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next? Taking RAG to Production\n",
    "\n",
    "What we built today is a **Naive RAG** system (per Gao et al. 2024 taxonomy). Here's how real products level up:\n",
    "\n",
    "| Level | Technique | Why |\n",
    "|-------|-----------|-----|\n",
    "| **Advanced RAG** | Query rewriting, reranking, HyDE | Better retrieval quality |\n",
    "| **Agentic RAG** | Multi-step retrieval, tool use | Handle complex multi-hop questions |\n",
    "| **Modular RAG** | Routing, adaptive retrieval | Only retrieve when needed |\n",
    "| **Evaluation** | RAGAS, faithfulness metrics | Measure and improve systematically |\n",
    "\n",
    "### Production Considerations for MBAs\n",
    "\n",
    "- **Cost**: Embedding is free (local model). LLM costs ~$0.001-0.01 per query.\n",
    "- **Latency**: Embedding ~50ms, search ~5ms, LLM ~1-3s. Total: ~2-4 seconds.\n",
    "- **Scale**: sqlite-vec handles millions of vectors. For billions, consider pgvector or Pinecone.\n",
    "- **Privacy**: Everything except the LLM call stays local. For full privacy, use local LLMs.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Lewis et al. (2020) — [RAG for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\n",
    "- Gao et al. (2024) — [RAG for LLMs: A Survey](https://arxiv.org/abs/2312.10997)\n",
    "- Wang et al. (2024) — [Improving Text Embeddings with LLMs](https://arxiv.org/abs/2401.00368)\n",
    "- Muennighoff et al. (2023) — [MTEB: Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316)\n",
    "- [sqlite-vec documentation](https://alexgarcia.xyz/sqlite-vec/)\n",
    "- [fastembed documentation](https://qdrant.github.io/fastembed/)\n",
    "\n",
    "---\n",
    "*Workshop created for [MIT AI Studio](https://aiforimpact.github.io/) (MAS.664/665, EC.731, IDS.865) — Spring 2026*\n",
    "\n",
    "*Instructor: [Brandon Sneider](https://linkedin.com/in/brandonsneider)*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai-studio-rag-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}