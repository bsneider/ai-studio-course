{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='text-align:center; padding: 20px 0;'>\n",
    "<h1 style='color:#A31F34; margin-bottom:0;'>MIT AI STUDIO COURSE</h1>\n",
    "<h3 style='color:#8A8B8C; margin-top:5px;'>MAS.664 / MAS.665 / EC.731 / IDS.865</h3>\n",
    "<h2>RAG Workshop: Chat with the Course Website</h2>\n",
    "<p style='font-size:14px; color:#666;'>Lead Professor: Ramesh Raskar, MIT Media Lab</p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "**Workshop Instructor:** [Brandon Sneider](https://linkedin.com/in/brandonsneider) — AI Manager at a defense technology startup, building AI systems for highly regulated environments. Brandon brings practical experience deploying LLMs and RAG pipelines where accuracy, compliance, and auditability are non-negotiable.\n",
    "\n",
    "---\n",
    "\n",
    "In this **30-minute hands-on workshop**, you'll build a **Retrieval-Augmented Generation (RAG)** system that crawls the [AI Studio course website](https://aiforimpact.github.io/), stores it in a vector database, and lets you **chat with the data** — all inside this notebook.\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "Large Language Models (LLMs) are powerful but have two critical limitations:\n",
    "1. **Knowledge cutoff** — they don't know about data after their training date\n",
    "2. **Hallucination** — they confidently generate plausible but incorrect information\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** solves both by retrieving relevant documents from a knowledge base and injecting them into the LLM's prompt as context before generating an answer.\n",
    "\n",
    "```\n",
    "User Question → [Retrieve relevant docs] → [Inject into prompt] → LLM → Grounded Answer\n",
    "```\n",
    "\n",
    "### Key Academic References\n",
    "\n",
    "| Paper | Key Contribution |\n",
    "|-------|------------------|\n",
    "| Lewis et al. (2020) \"[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\" — *NeurIPS 2020* | Introduced RAG: combines retriever + generator, reducing hallucination and improving factual accuracy |\n",
    "| Karpukhin et al. (2020) \"[Dense Passage Retrieval for Open-Domain QA](https://arxiv.org/abs/2004.04906)\" — *EMNLP 2020* | Dense embeddings outperform BM25 keyword search for passage retrieval |\n",
    "| Robertson & Zaragoza (2009) \"[The Probabilistic Relevance Framework: BM25 and Beyond](https://doi.org/10.1561/1500000019)\" | Foundational work on BM25 scoring |\n",
    "| Gao et al. (2024) \"[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)\" | Comprehensive survey: Naive RAG, Advanced RAG, Modular RAG |\n",
    "| Muennighoff et al. (2023) \"[MTEB: Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316)\" | Benchmark for comparing embedding models across 56 datasets |\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "By the end of this workshop, you will be able to:\n",
    "\n",
    "1. **Explain** what RAG is and why it matters for business (reducing hallucination, grounding LLMs in private data)\n",
    "2. **Compare** embedding models and understand when to fine-tune vs use off-the-shelf\n",
    "3. **Build** a document ingestion pipeline that crawls a website, chunks content, and generates embeddings\n",
    "4. **Understand** the difference between BM25 keyword search and semantic vector search, and why hybrid search outperforms either alone\n",
    "5. **Implement** a working RAG chatbot backed by SQLite + sqlite-vec\n",
    "6. **Evaluate** RAG quality and use AI to tune hyperparameters\n",
    "\n",
    "### Tech Stack\n",
    "\n",
    "| Component | Technology | Why |\n",
    "|-----------|-----------|-----|\n",
    "| Database | **SQLite + sqlite-vec** | Zero infrastructure, runs anywhere, scales to millions of vectors |\n",
    "| Keyword Search | **FTS5 (BM25)** | Built into SQLite, great for exact term matching |\n",
    "| Vector Search | **sqlite-vec** | Native cosine similarity, compact binary storage |\n",
    "| Embeddings | **MiniLM-L6-v2** (local) | No API key needed, runs on CPU, 384 dimensions |\n",
    "| LLM | **OpenRouter** | Access to frontier models, OpenAI-compatible API |\n",
    "| Web Scraping | **BeautifulSoup** | Parse the AI Studio website HTML |\n",
    "| Chat UI | **IPython widgets** | Interactive chat right in the notebook |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Setup & Installation (2 min)\n",
    "\n",
    "We install everything we need in one cell. This takes ~30 seconds on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install all dependencies (run once)\n",
    "!pip install -q sqlite-vec fastembed numpy openai requests beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import struct\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import sqlite_vec\n",
    "from bs4 import BeautifulSoup\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "print(\"All libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Initialize the Vector Database (3 min)\n",
    "\n",
    "We use **SQLite** with two extensions:\n",
    "- **FTS5** — Full-Text Search for BM25 keyword matching (built into SQLite)\n",
    "- **sqlite-vec** — Vector similarity search using cosine distance\n",
    "\n",
    "This gives us a **hybrid search** system in a single file — no external database servers needed.\n",
    "\n",
    "> **Why hybrid?** Karpukhin et al. (2020) showed dense retrieval beats BM25 for semantic queries,\n",
    "> but BM25 still wins for exact name/term lookups. Combining both gets the best of each.\n",
    "\n",
    "| Query Example | BM25 (keyword) | Semantic (vector) | Best Approach |\n",
    "|--------------|:-:|:-:|---|\n",
    "| \"Ramesh Raskar\" | Exact match | May miss | **BM25** |\n",
    "| \"Who teaches AI ethics?\" | No exact terms | Understands meaning | **Semantic** |\n",
    "| \"MIT Media Lab AI course\" | Partial match | Related concepts | **Hybrid** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_PATH = Path(\"ai_studio_rag.db\")\n",
    "\n",
    "def init_database(db_path: Path) -> sqlite3.Connection:\n",
    "    \"\"\"Create database with embeddings table, FTS5 for BM25, and vec0 for vectors.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    conn.row_factory = sqlite3.Row\n",
    "\n",
    "    # Load sqlite-vec extension\n",
    "    conn.enable_load_extension(True)\n",
    "    sqlite_vec.load(conn)\n",
    "    conn.enable_load_extension(False)\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Metadata table — stores the actual content and source info\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS documents (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            url TEXT NOT NULL,\n",
    "            page_title TEXT,\n",
    "            section_title TEXT,\n",
    "            content TEXT NOT NULL,\n",
    "            content_type TEXT DEFAULT 'text',\n",
    "            metadata TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # Vector table — 384 dimensions for MiniLM-L6-v2 embeddings\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE VIRTUAL TABLE IF NOT EXISTS vec_documents USING vec0(\n",
    "            embedding float[384] distance_metric=cosine\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # FTS5 table — for BM25 keyword search\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(\n",
    "            content,\n",
    "            page_title,\n",
    "            section_title,\n",
    "            content='documents',\n",
    "            content_rowid='id'\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # Triggers to keep FTS5 in sync automatically\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TRIGGER IF NOT EXISTS docs_ai AFTER INSERT ON documents BEGIN\n",
    "            INSERT INTO documents_fts(rowid, content, page_title, section_title)\n",
    "            VALUES (new.id, new.content, new.page_title, new.section_title);\n",
    "        END\n",
    "    \"\"\")\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TRIGGER IF NOT EXISTS docs_ad AFTER DELETE ON documents BEGIN\n",
    "            INSERT INTO documents_fts(documents_fts, rowid, content, page_title, section_title)\n",
    "            VALUES ('delete', old.id, old.content, old.page_title, old.section_title);\n",
    "        END\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "# Initialize!\n",
    "db = init_database(DATABASE_PATH)\n",
    "print(f\"Database created at: {DATABASE_PATH}\")\n",
    "print(f\"sqlite-vec loaded — ready for vector search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Crawl the AI Studio Website (5 min)\n",
    "\n",
    "We'll scrape every page of [aiforimpact.github.io](https://aiforimpact.github.io/) including:\n",
    "- Course overview and pillars\n",
    "- Speaker & mentor bios (40+ people)\n",
    "- Schedule and registration info\n",
    "- Past semester archives (Spring 2024 → Spring 2026)\n",
    "- Embedded YouTube video references\n",
    "- Image descriptions\n",
    "\n",
    "### The RAG Ingestion Pipeline\n",
    "\n",
    "```\n",
    "Website → Crawl HTML → Extract & Clean → Chunk by Section → Embed → Store in SQLite\n",
    "```\n",
    "\n",
    "**Chunking strategy**: We split by HTML sections (headers), keeping each chunk semantically coherent.\n",
    "This matters — Gao et al. (2024) show that chunk size and boundary selection significantly impact retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Website Crawler ──────────────────────────────────────────────────────────\n",
    "\n",
    "SITE_PAGES = [\n",
    "    \"https://aiforimpact.github.io/\",\n",
    "    \"https://aiforimpact.github.io/spring26.html\",\n",
    "    \"https://aiforimpact.github.io/fall25.html\",\n",
    "    \"https://aiforimpact.github.io/spring25.html\",\n",
    "    \"https://aiforimpact.github.io/fall24.html\",\n",
    "    \"https://aiforimpact.github.io/spring24.html\",\n",
    "    \"https://aiforimpact.github.io/fall23.html\",\n",
    "]\n",
    "\n",
    "def crawl_page(url: str) -> dict:\n",
    "    \"\"\"Fetch and parse a single page, extracting structured content.\"\"\"\n",
    "    resp = requests.get(url, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "    title = soup.title.string.strip() if soup.title else url.split('/')[-1]\n",
    "\n",
    "    # Remove script and style elements\n",
    "    for tag in soup([\"script\", \"style\", \"nav\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    # ── Extract speaker/mentor bios ──\n",
    "    for card in soup.select(\".card, .speaker-card, .col-md-3, .col-lg-3\"):\n",
    "        img = card.find(\"img\")\n",
    "        text_parts = [t.strip() for t in card.stripped_strings]\n",
    "        if text_parts and len(text_parts) >= 2:\n",
    "            name = text_parts[0]\n",
    "            role = \" \".join(text_parts[1:])\n",
    "            img_url = urljoin(url, img[\"src\"]) if img and img.get(\"src\") else None\n",
    "            bio_text = f\"Speaker/Mentor: {name}. Role: {role}.\"\n",
    "            if img_url:\n",
    "                bio_text += f\" Photo: {img_url}\"\n",
    "            chunks.append({\n",
    "                \"content\": bio_text,\n",
    "                \"section_title\": f\"Bio: {name}\",\n",
    "                \"content_type\": \"bio\",\n",
    "                \"metadata\": {\"name\": name, \"role\": role, \"image_url\": img_url}\n",
    "            })\n",
    "\n",
    "    # ── Extract YouTube videos ──\n",
    "    for iframe in soup.find_all(\"iframe\"):\n",
    "        src = iframe.get(\"src\", \"\")\n",
    "        if \"youtube\" in src or \"youtu.be\" in src:\n",
    "            video_title = iframe.get(\"title\", \"Course Video\")\n",
    "            chunks.append({\n",
    "                \"content\": f\"Video: {video_title}. URL: {src}. This is an embedded video from the AI Studio course page.\",\n",
    "                \"section_title\": f\"Video: {video_title}\",\n",
    "                \"content_type\": \"video\",\n",
    "                \"metadata\": {\"video_url\": src, \"title\": video_title}\n",
    "            })\n",
    "\n",
    "    # ── Extract images with context ──\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        src = img.get(\"src\", \"\")\n",
    "        alt = img.get(\"alt\", \"\")\n",
    "        if src and not src.startswith(\"data:\") and alt:\n",
    "            img_url = urljoin(url, src)\n",
    "            parent_text = img.parent.get_text(strip=True)[:200] if img.parent else \"\"\n",
    "            chunks.append({\n",
    "                \"content\": f\"Image: {alt}. URL: {img_url}. Context: {parent_text}\",\n",
    "                \"section_title\": f\"Image: {alt[:50]}\",\n",
    "                \"content_type\": \"image\",\n",
    "                \"metadata\": {\"image_url\": img_url, \"alt_text\": alt}\n",
    "            })\n",
    "\n",
    "    # ── Extract main text content by sections ──\n",
    "    main = soup.find(\"main\") or soup.find(\"body\")\n",
    "    if main:\n",
    "        current_section = \"Overview\"\n",
    "        current_text = []\n",
    "\n",
    "        for element in main.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"p\", \"li\", \"td\", \"th\"]):\n",
    "            if element.name in [\"h1\", \"h2\", \"h3\"]:\n",
    "                if current_text:\n",
    "                    text = \"\\n\".join(current_text).strip()\n",
    "                    if len(text) > 30:\n",
    "                        chunks.append({\n",
    "                            \"content\": f\"{current_section}\\n\\n{text}\",\n",
    "                            \"section_title\": current_section,\n",
    "                            \"content_type\": \"text\",\n",
    "                            \"metadata\": {}\n",
    "                        })\n",
    "                current_section = element.get_text(strip=True)\n",
    "                current_text = []\n",
    "            else:\n",
    "                text = element.get_text(strip=True)\n",
    "                if text and len(text) > 5:\n",
    "                    current_text.append(text)\n",
    "\n",
    "        if current_text:\n",
    "            text = \"\\n\".join(current_text).strip()\n",
    "            if len(text) > 30:\n",
    "                chunks.append({\n",
    "                    \"content\": f\"{current_section}\\n\\n{text}\",\n",
    "                    \"section_title\": current_section,\n",
    "                    \"content_type\": \"text\",\n",
    "                    \"metadata\": {}\n",
    "                })\n",
    "\n",
    "    return {\"url\": url, \"title\": title, \"chunks\": chunks}\n",
    "\n",
    "\n",
    "def crawl_site(pages: list[str]) -> list[dict]:\n",
    "    \"\"\"Crawl all pages and return flat list of chunks.\"\"\"\n",
    "    all_chunks = []\n",
    "    for url in pages:\n",
    "        print(f\"  Crawling: {url}\")\n",
    "        try:\n",
    "            page = crawl_page(url)\n",
    "            for chunk in page[\"chunks\"]:\n",
    "                chunk[\"url\"] = url\n",
    "                chunk[\"page_title\"] = page[\"title\"]\n",
    "            all_chunks.extend(page[\"chunks\"])\n",
    "            print(f\"    -> {len(page['chunks'])} chunks extracted\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error: {e}\")\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "print(\"Crawling the AI Studio website...\\n\")\n",
    "all_chunks = crawl_site(SITE_PAGES)\n",
    "\n",
    "# Summary\n",
    "types = {}\n",
    "for c in all_chunks:\n",
    "    types[c[\"content_type\"]] = types.get(c[\"content_type\"], 0) + 1\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Total chunks extracted: {len(all_chunks)}\")\n",
    "for t, count in sorted(types.items()):\n",
    "    print(f\"  {t}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Embedding Models Deep Dive & Store (5 min)\n",
    "\n",
    "Now we convert each text chunk into a **384-dimensional vector** using the MiniLM-L6-v2 model.\n",
    "\n",
    "**How embeddings work:**\n",
    "- Text → Neural network → Dense vector (array of 384 numbers)\n",
    "- Semantically similar texts produce vectors that are **close together** in vector space\n",
    "- \"Who teaches the course?\" and \"Professor Ramesh Raskar leads the class\" will have similar embeddings, even though they share few words\n",
    "\n",
    "### Why MiniLM-L6-v2? Embedding Model Comparison\n",
    "\n",
    "Choosing an embedding model is one of the most impactful decisions in a RAG system. Here's how the major options compare:\n",
    "\n",
    "| Model | Dims | Size | Cost | Speed | Multilingual | Best For |\n",
    "|-------|------|------|------|-------|-------------|----------|\n",
    "| **all-MiniLM-L6-v2** (ours) | 384 | 80 MB | Free (local) | ~50ms | English only | Prototyping, workshops, CPU |\n",
    "| **BGE-M3** (BAAI) | 1024 | 2.2 GB | Free (local) | ~200ms | 100+ languages | Production multilingual RAG |\n",
    "| **Amazon Titan Embeddings** | 1024 | API | $0.0001/1K tok | ~100ms | 25+ languages | AWS-integrated pipelines |\n",
    "| **OpenAI text-embedding-3-large** | 3072 | API | $0.00013/1K tok | ~80ms | Good | Highest quality, cost-tolerant |\n",
    "| **Cohere embed-v3** | 1024 | API | $0.0001/1K tok | ~90ms | 100+ languages | Search-optimized |\n",
    "\n",
    "**We chose MiniLM-L6-v2 because:**\n",
    "1. **Zero cost** — no API key needed for embeddings, runs entirely on CPU\n",
    "2. **Fast** — smallest model, perfect for a 30-min workshop\n",
    "3. **Good enough** — for English-only <100K docs, quality is comparable to larger models\n",
    "4. **384 dimensions** — smaller vectors = faster search, less storage (vs 1024 or 3072)\n",
    "\n",
    "> **When to upgrade:** For multilingual, use BGE-M3. For max quality, use OpenAI's `text-embedding-3-large`.\n",
    "> See Muennighoff et al. (2023) \"[MTEB: Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316)\" for comprehensive benchmarks.\n",
    "\n",
    "### When and How to Fine-Tune an Embedding Model\n",
    "\n",
    "**When fine-tuning helps** (Wang et al. 2024, \"[Improving Text Embeddings with LLMs](https://arxiv.org/abs/2401.00368)\"):\n",
    "- Your domain has **specialized vocabulary** (legal, medical, finance) that general models don't capture\n",
    "- You have **query-document pairs** showing what users search for vs what they should find\n",
    "- General models score below ~0.7 accuracy on your test queries\n",
    "- Example: A legal RAG where \"consideration\" means contract terms, not \"thinking about\"\n",
    "\n",
    "**When fine-tuning is NOT worth it:**\n",
    "- Your content is general English (like our course website)\n",
    "- You have fewer than ~1,000 labeled query-document pairs\n",
    "- Switching to a larger pre-trained model would solve the problem\n",
    "- You're still iterating on chunking strategy or retrieval logic\n",
    "\n",
    "**How fine-tuning works (conceptual):**\n",
    "```\n",
    "1. Collect pairs:    (query, relevant_document, irrelevant_document)\n",
    "2. Contrastive loss: Push relevant pairs closer, irrelevant pairs apart\n",
    "3. Result:           Model now \"understands\" your domain's similarity\n",
    "```\n",
    "\n",
    "Tools: [Sentence-Transformers fine-tuning](https://www.sbert.net/docs/training/overview.html), or generate synthetic pairs with LLMs (Wang et al. 2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model (downloads ~80MB on first run)\n",
    "print(\"Loading MiniLM-L6-v2 embedding model (384 dimensions, ONNX runtime)...\")\n",
    "embedding_model = TextEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Model loaded!\\n\")\n",
    "\n",
    "\n",
    "def generate_embedding(text: str) -> list[float]:\n",
    "    \"\"\"Generate a 384-dim embedding for a single text.\"\"\"\n",
    "    return list(embedding_model.embed([text]))[0].tolist()\n",
    "\n",
    "\n",
    "def generate_embeddings_batch(texts: list[str]) -> list[list[float]]:\n",
    "    \"\"\"Batch embed for efficiency.\"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "    return [emb.tolist() for emb in embedding_model.embed(texts)]\n",
    "\n",
    "\n",
    "def serialize_embedding(embedding: list[float]) -> bytes:\n",
    "    \"\"\"Pack embedding as binary for sqlite-vec.\"\"\"\n",
    "    return struct.pack(f'{len(embedding)}f', *embedding)\n",
    "\n",
    "\n",
    "# ── Quick demo: see how embeddings capture meaning ──\n",
    "demo_texts = [\n",
    "    \"Professor Raskar teaches AI at MIT\",\n",
    "    \"The lead instructor of the course is from Media Lab\",\n",
    "    \"I had pizza for lunch today\",\n",
    "]\n",
    "demo_embs = generate_embeddings_batch(demo_texts)\n",
    "\n",
    "print(\"Embedding similarity demo:\")\n",
    "print(f\"  Text A: '{demo_texts[0]}'\")\n",
    "print(f\"  Text B: '{demo_texts[1]}'  (related)\")\n",
    "print(f\"  Text C: '{demo_texts[2]}'  (unrelated)\")\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "def cosine_sim(a, b): return dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "print(f\"\\n  Similarity A<->B (related):   {cosine_sim(demo_embs[0], demo_embs[1]):.3f}\")\n",
    "print(f\"  Similarity A<->C (unrelated): {cosine_sim(demo_embs[0], demo_embs[2]):.3f}\")\n",
    "print(f\"  (Higher = more similar. Related texts cluster together!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Ingest all chunks into the database ──\n",
    "print(\"Embedding and storing all chunks...\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "total_stored = 0\n",
    "\n",
    "for i in range(0, len(all_chunks), BATCH_SIZE):\n",
    "    batch = all_chunks[i:i + BATCH_SIZE]\n",
    "    texts = [c[\"content\"] for c in batch]\n",
    "    embeddings = generate_embeddings_batch(texts)\n",
    "\n",
    "    cursor = db.cursor()\n",
    "    for chunk, emb in zip(batch, embeddings):\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO documents (url, page_title, section_title, content, content_type, metadata)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            chunk[\"url\"],\n",
    "            chunk[\"page_title\"],\n",
    "            chunk[\"section_title\"],\n",
    "            chunk[\"content\"],\n",
    "            chunk[\"content_type\"],\n",
    "            json.dumps(chunk[\"metadata\"]),\n",
    "        ))\n",
    "        rowid = cursor.lastrowid\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO vec_documents (rowid, embedding) VALUES (?, ?)\n",
    "        \"\"\", (rowid, serialize_embedding(emb)))\n",
    "\n",
    "    db.commit()\n",
    "    total_stored += len(batch)\n",
    "    print(f\"  Stored {total_stored}/{len(all_chunks)} chunks...\")\n",
    "\n",
    "print(f\"\\nDone! {total_stored} chunks embedded and stored in {DATABASE_PATH}\")\n",
    "print(f\"Database size: {DATABASE_PATH.stat().st_size / 1024:.0f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Hybrid Search — BM25 + Semantic (5 min)\n",
    "\n",
    "This is the **retrieval** core of RAG. We implement two search strategies and combine them:\n",
    "\n",
    "### BM25 (Keyword Search)\n",
    "- Uses SQLite's **FTS5** extension\n",
    "- Scores based on term frequency / inverse document frequency\n",
    "- Great for exact names, acronyms, specific terms\n",
    "- Based on Robertson & Zaragoza (2009)\n",
    "\n",
    "### Semantic Search (Vector Similarity)\n",
    "- Uses **sqlite-vec**'s native cosine distance\n",
    "- Finds conceptually similar content even with different wording\n",
    "- Based on Karpukhin et al. (2020) Dense Passage Retrieval\n",
    "\n",
    "### Hybrid Fusion\n",
    "We normalize both scores to [0, 1] and combine with configurable weights:\n",
    "\n",
    "```\n",
    "final_score = keyword_weight * BM25_score + semantic_weight * cosine_similarity\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Search Functions ──────────────────────────────────────────────────────────\n",
    "\n",
    "def bm25_search(conn, query: str, limit: int = 50) -> dict[int, float]:\n",
    "    \"\"\"BM25 keyword search via FTS5. Returns {doc_id: score}.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    safe_query = query.replace('\"', '\"\"')\n",
    "    try:\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT rowid, bm25(documents_fts) as score\n",
    "            FROM documents_fts\n",
    "            WHERE documents_fts MATCH ?\n",
    "            LIMIT ?\n",
    "        \"\"\", (safe_query, limit))\n",
    "        return {row[0]: row[1] for row in cursor.fetchall()}\n",
    "    except sqlite3.OperationalError:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def semantic_search(conn, query_embedding: list[float], limit: int = 50) -> dict[int, float]:\n",
    "    \"\"\"Cosine similarity search via sqlite-vec. Returns {doc_id: distance}.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT rowid, distance\n",
    "        FROM vec_documents\n",
    "        WHERE embedding MATCH ? AND k = ?\n",
    "        ORDER BY distance\n",
    "    \"\"\", (serialize_embedding(query_embedding), limit))\n",
    "    return {row[0]: row[1] for row in cursor.fetchall()}\n",
    "\n",
    "\n",
    "def normalize_scores(scores: dict[int, float], higher_is_better: bool = True) -> dict[int, float]:\n",
    "    \"\"\"Normalize scores to [0, 1].\"\"\"\n",
    "    if not scores:\n",
    "        return {}\n",
    "    vals = list(scores.values())\n",
    "    min_v, max_v = min(vals), max(vals)\n",
    "    if min_v == max_v:\n",
    "        return {k: 1.0 for k in scores}\n",
    "    if higher_is_better:\n",
    "        return {k: (v - min_v) / (max_v - min_v) for k, v in scores.items()}\n",
    "    else:\n",
    "        return {k: (max_v - v) / (max_v - min_v) for k, v in scores.items()}\n",
    "\n",
    "\n",
    "def hybrid_search(\n",
    "    conn,\n",
    "    query: str,\n",
    "    query_embedding: list[float],\n",
    "    keyword_weight: float = 0.3,\n",
    "    semantic_weight: float = 0.7,\n",
    "    top_k: int = 5,\n",
    ") -> list[dict]:\n",
    "    \"\"\"Combine BM25 + semantic search with weighted fusion.\"\"\"\n",
    "    bm25_raw = bm25_search(conn, query)\n",
    "    bm25_norm = normalize_scores(bm25_raw, higher_is_better=False)\n",
    "\n",
    "    sem_raw = semantic_search(conn, query_embedding)\n",
    "    sem_norm = normalize_scores(sem_raw, higher_is_better=False)\n",
    "\n",
    "    all_ids = set(bm25_norm.keys()) | set(sem_norm.keys())\n",
    "    if not all_ids:\n",
    "        return []\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    placeholders = \",\".join(\"?\" * len(all_ids))\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT id, url, page_title, section_title, content, content_type, metadata\n",
    "        FROM documents WHERE id IN ({placeholders})\n",
    "    \"\"\", list(all_ids))\n",
    "    cols = [\"id\",\"url\",\"page_title\",\"section_title\",\"content\",\"content_type\",\"metadata\"]\n",
    "    docs = {row[0]: dict(zip(cols, row)) for row in cursor.fetchall()}\n",
    "\n",
    "    results = []\n",
    "    for doc_id in all_ids:\n",
    "        bm25_s = bm25_norm.get(doc_id, 0.0)\n",
    "        sem_s = sem_norm.get(doc_id, 0.0)\n",
    "        final = keyword_weight * bm25_s + semantic_weight * sem_s\n",
    "        doc = docs.get(doc_id, {})\n",
    "        results.append({**doc, \"bm25_score\": bm25_s, \"semantic_score\": sem_s, \"final_score\": final})\n",
    "\n",
    "    results.sort(key=lambda x: x[\"final_score\"], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "\n",
    "# ── Test it! ──\n",
    "test_queries = [\n",
    "    \"Who is the lead professor of the course?\",\n",
    "    \"What are the course pillars?\",\n",
    "    \"Tell me about the guest speakers from venture capital\",\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {q}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    emb = generate_embedding(q)\n",
    "    results = hybrid_search(db, q, emb, top_k=3)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"  {i}. [{r.get('content_type','?')}] Score: {r['final_score']:.3f} \"\n",
    "              f\"(BM25: {r['bm25_score']:.2f}, Semantic: {r['semantic_score']:.2f})\")\n",
    "        content_preview = r.get('content', '')[:120].replace('\\n', ' ')\n",
    "        print(f\"     {content_preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Connect the LLM for Generation (3 min)\n",
    "\n",
    "Now we add the **G** in RA**G** — we take retrieved context and use an LLM to generate a grounded answer.\n",
    "\n",
    "We use **OpenRouter** which provides access to frontier models via an OpenAI-compatible API.\n",
    "\n",
    "### The RAG Prompt Pattern\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant for the MIT AI Studio course.\n",
    "        Answer ONLY using the provided context.\n",
    "\n",
    "User:   Context: [retrieved documents]\n",
    "        Question: [user's question]\n",
    "```\n",
    "\n",
    "This is the key insight from Lewis et al. (2020): by conditioning generation on retrieved evidence, the model produces **factual, verifiable answers** rather than hallucinating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from openai import OpenAI\n",
    "\n",
    "# ── Get API key from Colab Secrets ──\n",
    "# Go to the key icon in the left sidebar -> add OPENROUTER_API_KEY\n",
    "# Get a free key at https://openrouter.ai/keys\n",
    "try:\n",
    "    api_key = userdata.get(\"OPENROUTER_API_KEY\")\n",
    "except Exception:\n",
    "    api_key = None\n",
    "\n",
    "if not api_key:\n",
    "    api_key = input(\"Enter your OpenRouter API key (get one at https://openrouter.ai/keys): \").strip()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=api_key,\n",
    ")\n",
    "print(\"LLM client connected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── RAG Answer Function ───────────────────────────────────────────────────────\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant for the MIT AI Studio course\n",
    "(MAS.664 / MAS.665 / EC.731 / IDS.865), taught at MIT Media Lab by Professor Ramesh Raskar.\n",
    "\n",
    "Rules:\n",
    "- Answer ONLY using the provided context. Do not use outside knowledge.\n",
    "- If the context mentions images or videos, include the URLs so the user can view them.\n",
    "- If the context doesn't contain enough information, say so clearly.\n",
    "- Be concise but thorough. Use bullet points for lists.\n",
    "- When mentioning people, include their role/affiliation if available.\"\"\"\n",
    "\n",
    "\n",
    "def format_context(results: list[dict], max_chars: int = 4000) -> str:\n",
    "    \"\"\"Format retrieved documents into context for the LLM.\"\"\"\n",
    "    if not results:\n",
    "        return \"No relevant documents found.\"\n",
    "    parts = []\n",
    "    chars = 0\n",
    "    for i, r in enumerate(results, 1):\n",
    "        entry = f\"[Source {i}: {r.get('page_title', '?')} > {r.get('section_title', '?')}]\\n{r.get('content', '')}\"\n",
    "        if chars + len(entry) > max_chars:\n",
    "            break\n",
    "        parts.append(entry)\n",
    "        chars += len(entry)\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "def rag_answer(question: str, conn=db, top_k: int = 5) -> tuple[str, list[dict]]:\n",
    "    \"\"\"Full RAG pipeline: retrieve -> format -> generate.\"\"\"\n",
    "    query_emb = generate_embedding(question)\n",
    "    results = hybrid_search(conn, question, query_emb, top_k=top_k)\n",
    "    context = format_context(results)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"google/gemini-2.0-flash-001\",\n",
    "        max_tokens=800,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"},\n",
    "        ],\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    return answer, results\n",
    "\n",
    "\n",
    "# ── Quick test ──\n",
    "question = \"Who are the lead instructors and what is the course about?\"\n",
    "answer, sources = rag_answer(question)\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(f\"A: {answer}\\n\")\n",
    "print(f\"Sources used: {len(sources)} documents\")\n",
    "for s in sources:\n",
    "    print(f\"  - [{s.get('content_type','')}] {s.get('section_title', '')[:60]} (score: {s['final_score']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Interactive Chat Interface (5 min)\n",
    "\n",
    "Now let's build an interactive chat — right here in the notebook!\n",
    "\n",
    "Try asking:\n",
    "- \"Who are the venture capital speakers?\"\n",
    "- \"What's the difference between Spring 2025 and Fall 2025?\"\n",
    "- \"What are the course pillars?\"\n",
    "- \"Tell me about the demo day\"\n",
    "- \"Are there any videos I can watch?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── In-Notebook Chat UI ───────────────────────────────────────────────────────\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "chat_output = widgets.Output(layout=widgets.Layout(\n",
    "    width='100%', min_height='300px', max_height='500px',\n",
    "    overflow_y='auto', border='1px solid #444', padding='10px',\n",
    "))\n",
    "input_box = widgets.Text(\n",
    "    placeholder='Ask anything about the MIT AI Studio course...',\n",
    "    layout=widgets.Layout(width='85%'),\n",
    ")\n",
    "send_btn = widgets.Button(description='Send', button_style='primary',\n",
    "                          layout=widgets.Layout(width='14%'))\n",
    "show_sources = widgets.Checkbox(value=False, description='Show sources', indent=False)\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def render_chat():\n",
    "    with chat_output:\n",
    "        clear_output()\n",
    "        for msg in chat_history:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                display(HTML(f\"<div style='margin:8px 0;padding:10px 14px;background:#1a3a5c;\"\n",
    "                    f\"border-radius:12px;color:white;max-width:80%;margin-left:auto;\"\n",
    "                    f\"text-align:right;font-size:14px;'><b>You:</b> {msg['content']}</div>\"))\n",
    "            else:\n",
    "                sources_html = \"\"\n",
    "                if show_sources.value and msg.get(\"sources\"):\n",
    "                    src_items = \"\".join(\n",
    "                        f\"<br>- [{s.get('content_type','')}] {s.get('section_title','')[:50]} (score: {s['final_score']:.2f})\"\n",
    "                        for s in msg[\"sources\"]\n",
    "                    )\n",
    "                    sources_html = f\"<br><details><summary><small>Sources</small></summary><small>{src_items}</small></details>\"\n",
    "                display(HTML(f\"<div style='margin:8px 0;padding:10px 14px;background:#2d2d2d;\"\n",
    "                    f\"border-radius:12px;color:#e0e0e0;max-width:90%;font-size:14px;\"\n",
    "                    f\"line-height:1.5;'><b>AI Studio Assistant:</b><br>{msg['content']}{sources_html}</div>\"))\n",
    "\n",
    "def on_send(btn):\n",
    "    question = input_box.value.strip()\n",
    "    if not question:\n",
    "        return\n",
    "    input_box.value = \"\"\n",
    "    chat_history.append({\"role\": \"user\", \"content\": question})\n",
    "    render_chat()\n",
    "    with chat_output:\n",
    "        display(HTML(\"<div style='color:#888;padding:5px;font-style:italic;'>Searching & generating...</div>\"))\n",
    "    try:\n",
    "        answer, sources = rag_answer(question)\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": answer, \"sources\": sources})\n",
    "    except Exception as e:\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": f\"Error: {e}\", \"sources\": []})\n",
    "    render_chat()\n",
    "\n",
    "send_btn.on_click(on_send)\n",
    "input_box.on_submit(lambda _: on_send(None))\n",
    "show_sources.observe(lambda _: render_chat(), names='value')\n",
    "\n",
    "header = widgets.HTML(\"<h3 style='margin:0;padding:8px 0;'>Chat with the AI Studio Course</h3>\")\n",
    "display(widgets.VBox([header, chat_output, widgets.HBox([input_box, send_btn]), widgets.HBox([show_sources])]))\n",
    "\n",
    "chat_history.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Welcome! I've ingested the entire <b>MIT AI Studio course website</b> \"\n",
    "               \"(7 pages, all semesters, 40+ speaker bios, videos, and images). \"\n",
    "               \"Ask me anything!<br><br>\"\n",
    "               \"<b>Try:</b> 'Who are the VC speakers?' or 'What is the course about?' \"\n",
    "               \"or 'How do I register for Spring 2026?'\",\n",
    "    \"sources\": []\n",
    "})\n",
    "render_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Understanding What Just Happened — RAG Recap\n",
    "\n",
    "Let's trace what happens when you type a question in the chat:\n",
    "\n",
    "```\n",
    " 1. EMBED        Your question -> 384-dim vector\n",
    "                 \"Who teaches AI ethics?\" -> [0.03, -0.12, ...]\n",
    "\n",
    " 2. RETRIEVE     Hybrid search over the database\n",
    "                 BM25:     keyword match on \"teaches\", \"AI\"\n",
    "                 Semantic: cosine similarity to find related\n",
    "                 -> Top 5 most relevant chunks\n",
    "\n",
    " 3. AUGMENT      Inject retrieved chunks into the LLM prompt\n",
    "                 \"Context: [Source 1: ...] [Source 2: ...]\"\n",
    "\n",
    " 4. GENERATE     LLM produces answer grounded in evidence\n",
    "                 -> Factual, verifiable, no hallucination\n",
    "```\n",
    "\n",
    "### Why RAG Matters for Business (MBA Perspective)\n",
    "\n",
    "| Use Case | Without RAG | With RAG |\n",
    "|----------|-------------|----------|\n",
    "| Customer support chatbot | Generic answers, can't access your docs | Answers from your actual knowledge base |\n",
    "| Legal document review | May hallucinate case law | Grounded in your actual contracts |\n",
    "| Internal company wiki Q&A | Outdated training data | Always current with your docs |\n",
    "| Due diligence research | May confuse companies | Grounded in actual filings |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **RAG = Retrieve + Augment + Generate** — give the LLM the right context, get grounded answers\n",
    "2. **Hybrid search > either alone** — combine keyword (BM25) and semantic (vector) for best results\n",
    "3. **Embeddings are the bridge** — they convert text to numbers so we can measure similarity\n",
    "4. **Model choice matters** — MiniLM for prototypes, BGE-M3/OpenAI for production\n",
    "5. **The entire system runs locally** — embeddings are local, only the LLM call needs an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Compare Search Strategies\n",
    "Run the cell below to see how BM25-only, semantic-only, and hybrid search differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Compare search strategies for different query types\n",
    "\n",
    "test_cases = [\n",
    "    (\"Ramesh Raskar\", \"Exact name lookup - BM25 should excel\"),\n",
    "    (\"venture capital investors in AI\", \"Conceptual query - semantic should excel\"),\n",
    "    (\"MIT Media Lab course spring 2026\", \"Mix of specific + conceptual - hybrid wins\"),\n",
    "]\n",
    "\n",
    "for query, description in test_cases:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Query: \\\"{query}\\\"\")\n",
    "    print(f\"Expected: {description}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    emb = generate_embedding(query)\n",
    "\n",
    "    r_bm25 = hybrid_search(db, query, emb, keyword_weight=1.0, semantic_weight=0.0, top_k=3)\n",
    "    r_sem = hybrid_search(db, query, emb, keyword_weight=0.0, semantic_weight=1.0, top_k=3)\n",
    "    r_hyb = hybrid_search(db, query, emb, keyword_weight=0.3, semantic_weight=0.7, top_k=3)\n",
    "\n",
    "    for label, results in [(\"BM25 Only\", r_bm25), (\"Semantic Only\", r_sem), (\"Hybrid\", r_hyb)]:\n",
    "        print(f\"\\n  {label}:\")\n",
    "        if not results:\n",
    "            print(\"    (no results)\")\n",
    "        for r in results[:2]:\n",
    "            print(f\"    - {r.get('section_title', '?')[:50]} (score: {r['final_score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Add Your Own Content\n",
    "Add a new document to the knowledge base and query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Add your own content to the RAG system\n",
    "\n",
    "my_doc = \"\"\"\n",
    "Replace this with your own text! For example, paste your startup idea,\n",
    "a paragraph from a paper you're reading, or notes from a lecture.\n",
    "\"\"\"\n",
    "\n",
    "emb = generate_embedding(my_doc)\n",
    "cursor = db.cursor()\n",
    "cursor.execute(\"\"\"\n",
    "    INSERT INTO documents (url, page_title, section_title, content, content_type, metadata)\n",
    "    VALUES (?, ?, ?, ?, ?, ?)\n",
    "\"\"\", (\"user://custom\", \"My Document\", \"Custom Content\", my_doc, \"text\", \"{}\"))\n",
    "rowid = cursor.lastrowid\n",
    "cursor.execute(\"INSERT INTO vec_documents (rowid, embedding) VALUES (?, ?)\",\n",
    "               (rowid, serialize_embedding(emb)))\n",
    "db.commit()\n",
    "print(f\"Added your document (id={rowid}). Now try asking about it in the chat above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Let AI Tune Your RAG Hyperparameters\n",
    "\n",
    "Our RAG system has several \"knobs\" (hyperparameters) that affect quality. Right now they're set to reasonable defaults — but are they optimal for *this* dataset?\n",
    "\n",
    "**The hyperparameters we can tune:**\n",
    "- `keyword_weight` / `semantic_weight` — balance between BM25 and vector search\n",
    "- `top_k` — how many documents to retrieve\n",
    "\n",
    "Below, we first test with **untuned defaults**, then ask the LLM to analyze results and suggest better values. This is a real technique used in production RAG systems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Exercise 3: Untuned vs AI-Tuned Hyperparameters ─────────────────────────\n",
    "\n",
    "EVAL_QUERIES = [\n",
    "    {\"query\": \"Who is the lead professor?\",\n",
    "     \"expected_keywords\": [\"ramesh\", \"raskar\", \"media lab\"]},\n",
    "    {\"query\": \"What are the three course pillars?\",\n",
    "     \"expected_keywords\": [\"innovation\", \"human centered\", \"technical\"]},\n",
    "    {\"query\": \"Which venture capitalists are speakers?\",\n",
    "     \"expected_keywords\": [\"khosla\", \"lux\", \"pillar\"]},\n",
    "    {\"query\": \"How do I register for Spring 2026?\",\n",
    "     \"expected_keywords\": [\"questionnaire\", \"register\", \"step\"]},\n",
    "    {\"query\": \"What happens at demo day?\",\n",
    "     \"expected_keywords\": [\"demo\", \"presentations\", \"investors\"]},\n",
    "]\n",
    "\n",
    "def score_results(results: list[dict], expected_keywords: list[str]) -> float:\n",
    "    \"\"\"Score retrieval: what fraction of expected keywords appear in top results?\"\"\"\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    all_content = \" \".join(r.get(\"content\", \"\") for r in results).lower()\n",
    "    hits = sum(1 for kw in expected_keywords if kw.lower() in all_content)\n",
    "    return hits / len(expected_keywords)\n",
    "\n",
    "def evaluate_config(kw_w, sem_w, top_k):\n",
    "    \"\"\"Run all eval queries and return average retrieval score.\"\"\"\n",
    "    scores = []\n",
    "    for eq in EVAL_QUERIES:\n",
    "        emb = generate_embedding(eq[\"query\"])\n",
    "        results = hybrid_search(db, eq[\"query\"], emb,\n",
    "                               keyword_weight=kw_w, semantic_weight=sem_w, top_k=top_k)\n",
    "        scores.append(score_results(results, eq[\"expected_keywords\"]))\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "# ── Step 1: Test with UNTUNED defaults ──\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Testing with UNTUNED defaults\")\n",
    "print(\"  keyword_weight=0.3, semantic_weight=0.7, top_k=5\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "untuned_score = evaluate_config(0.3, 0.7, 5)\n",
    "print(f\"\\n  Overall retrieval score: {untuned_score:.1%}\\n\")\n",
    "\n",
    "for eq in EVAL_QUERIES:\n",
    "    emb = generate_embedding(eq[\"query\"])\n",
    "    results = hybrid_search(db, eq[\"query\"], emb, keyword_weight=0.3, semantic_weight=0.7, top_k=5)\n",
    "    s = score_results(results, eq[\"expected_keywords\"])\n",
    "    status = \"PASS\" if s >= 0.66 else \"WEAK\" if s > 0 else \"FAIL\"\n",
    "    print(f\"  [{status}] \\\"{eq['query']}\\\" -> {s:.0%} keywords found\")\n",
    "\n",
    "# ── Step 2: Ask AI to suggest better hyperparameters ──\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"STEP 2: Asking AI to analyze and suggest better config...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tuning_prompt = f\"\"\"You are optimizing a RAG system's hyperparameters.\n",
    "The system uses hybrid search combining BM25 keyword search and semantic vector search.\n",
    "\n",
    "Current config: keyword_weight=0.3, semantic_weight=0.7, top_k=5\n",
    "Current retrieval score: {untuned_score:.1%}\n",
    "\n",
    "Eval results:\n",
    "\"\"\"\n",
    "for eq in EVAL_QUERIES:\n",
    "    emb = generate_embedding(eq[\"query\"])\n",
    "    results = hybrid_search(db, eq[\"query\"], emb, keyword_weight=0.3, semantic_weight=0.7, top_k=5)\n",
    "    s = score_results(results, eq[\"expected_keywords\"])\n",
    "    top_sections = [r.get(\"section_title\", \"?\")[:40] for r in results[:3]]\n",
    "    tuning_prompt += (f\"\\n- Query: \\\"{eq['query']}\\\"\\n\"\n",
    "                      f\"  Expected keywords: {eq['expected_keywords']}\\n\"\n",
    "                      f\"  Score: {s:.0%}, Top results: {top_sections}\\n\")\n",
    "\n",
    "tuning_prompt += \"\"\"\\nSuggest improved hyperparameters. Consider:\n",
    "- If name lookups fail, increase keyword_weight\n",
    "- If conceptual queries fail, increase semantic_weight\n",
    "- If not enough context, increase top_k (max 10)\n",
    "\n",
    "Respond ONLY with JSON: {\"keyword_weight\": 0.4, \"semantic_weight\": 0.6, \"top_k\": 7, \"reasoning\": \"brief\"}\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"google/gemini-2.0-flash-001\",\n",
    "    max_tokens=200,\n",
    "    messages=[{\"role\": \"user\", \"content\": tuning_prompt}],\n",
    ")\n",
    "ai_response = response.choices[0].message.content.strip()\n",
    "print(f\"\\n  AI suggestion: {ai_response}\\n\")\n",
    "\n",
    "# Parse and test\n",
    "json_match = re.search(r'\\{[^}]+\\}', ai_response)\n",
    "if json_match:\n",
    "    suggestion = json.loads(json_match.group())\n",
    "    new_kw = suggestion.get(\"keyword_weight\", 0.4)\n",
    "    new_sem = suggestion.get(\"semantic_weight\", 0.6)\n",
    "    new_topk = min(suggestion.get(\"top_k\", 7), 10)\n",
    "\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"STEP 3: Testing with AI-TUNED config\")\n",
    "    print(f\"  keyword_weight={new_kw}, semantic_weight={new_sem}, top_k={new_topk}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    tuned_score = evaluate_config(new_kw, new_sem, new_topk)\n",
    "    print(f\"\\n  Overall retrieval score: {tuned_score:.1%}\\n\")\n",
    "\n",
    "    for eq in EVAL_QUERIES:\n",
    "        emb = generate_embedding(eq[\"query\"])\n",
    "        results = hybrid_search(db, eq[\"query\"], emb,\n",
    "                               keyword_weight=new_kw, semantic_weight=new_sem, top_k=new_topk)\n",
    "        s = score_results(results, eq[\"expected_keywords\"])\n",
    "        status = \"PASS\" if s >= 0.66 else \"WEAK\" if s > 0 else \"FAIL\"\n",
    "        print(f\"  [{status}] \\\"{eq['query']}\\\" -> {s:.0%} keywords found\")\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"SUMMARY\")\n",
    "    print(f\"  Untuned score:  {untuned_score:.1%}\")\n",
    "    print(f\"  AI-tuned score: {tuned_score:.1%}\")\n",
    "    delta = tuned_score - untuned_score\n",
    "    if delta > 0:\n",
    "        print(f\"  Improvement:    +{delta:.1%}\")\n",
    "    elif delta == 0:\n",
    "        print(f\"  No change (defaults were already good for this dataset)\")\n",
    "    else:\n",
    "        print(f\"  Change:         {delta:.1%} (AI suggestion didn't help here)\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "else:\n",
    "    print(\"  Could not parse AI suggestion. Try running the cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next? Taking RAG to Production\n",
    "\n",
    "What we built today is a **Naive RAG** system (per Gao et al. 2024 taxonomy). Here's how real products level up:\n",
    "\n",
    "| Level | Technique | Why |\n",
    "|-------|-----------|-----|\n",
    "| **Advanced RAG** | Query rewriting, reranking, HyDE | Better retrieval quality |\n",
    "| **Agentic RAG** | Multi-step retrieval, tool use | Handle complex multi-hop questions |\n",
    "| **Modular RAG** | Routing, adaptive retrieval | Only retrieve when needed |\n",
    "| **Evaluation** | RAGAS, faithfulness metrics | Measure and improve systematically |\n",
    "\n",
    "### Production Considerations for MBAs\n",
    "\n",
    "- **Cost**: Embedding is free (local model). LLM costs ~$0.001-0.01 per query.\n",
    "- **Latency**: Embedding ~50ms, search ~5ms, LLM ~1-3s. Total: ~2-4 seconds.\n",
    "- **Scale**: sqlite-vec handles millions of vectors. For billions, consider pgvector or Pinecone.\n",
    "- **Privacy**: Everything except the LLM call stays local. For full privacy, use local LLMs.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Lewis et al. (2020) — [RAG for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\n",
    "- Gao et al. (2024) — [RAG for LLMs: A Survey](https://arxiv.org/abs/2312.10997)\n",
    "- Wang et al. (2024) — [Improving Text Embeddings with LLMs](https://arxiv.org/abs/2401.00368)\n",
    "- Muennighoff et al. (2023) — [MTEB: Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316)\n",
    "- [sqlite-vec documentation](https://alexgarcia.xyz/sqlite-vec/)\n",
    "- [fastembed documentation](https://qdrant.github.io/fastembed/)\n",
    "\n",
    "---\n",
    "*Workshop created for [MIT AI Studio](https://aiforimpact.github.io/) (MAS.664/665, EC.731, IDS.865) — Spring 2026*\n",
    "\n",
    "*Instructor: [Brandon Sneider](https://linkedin.com/in/brandonsneider)*"
   ]
  }
 ]
}